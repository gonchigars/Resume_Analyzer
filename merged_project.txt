# Merged File Contents
# Generated on: 2025-02-24 10:02:12.451012
# Source Directory: C:\Users\gonch\Desktop\batch9-10\Resume_Analyzer


================================================================================
# File: .\.streamlit\config.toml
================================================================================

[server]
port = 8501


================================================================================
# File: .\About_Project\1.md
================================================================================

Main Purpose:

- Automatically analyzes resumes and compares them against specific job requirements
- Provides intelligent matching and scoring of candidates
- Manages a database of resumes for efficient searching and retrieval

Key Components:

1. Resume Processing:

- Can handle both PDF and text resume formats
- Breaks down resumes into searchable chunks using smart document processing
- Extracts metadata and key information from resumes

2. Analysis Features:

- Calculates a match score (0-100) between resumes and job requirements
- Identifies key matching qualifications
- Lists missing requirements
- Highlights additional relevant skills
- Calculates years of relevant experience
- Provides summary analysis of each candidate

3. Technical Stack:

- Built using Python with Streamlit for the user interface
- Uses OpenRouter's DeepSeek model for intelligent analysis
- Employs Pinecone for vector database storage and similarity searching
- Integrates with HuggingFace for embeddings and transformers
- Includes comprehensive logging and error handling

4. User Interface Features:

- Upload and manage multiple resumes
- Input job requirements
- View detailed analysis results
- Delete and manage stored resumes
- System status monitoring

The system seems particularly useful for:

- HR professionals and recruiters
- Hiring managers
- Companies with high-volume recruiting needs
- Anyone needing to efficiently screen and match candidates to job requirements


================================================================================
# File: .\About_Project\Architecture.mmd
================================================================================

graph TB
    subgraph Frontend
        UI[Streamlit UI]
        UI --> |User Input| APP[App Controller]
    end

    subgraph Core Services
        APP --> |Initialize| RA[Resume Analyzer]
        RA --> |Process| DP[Document Processor]
        RA --> |Store/Query| VS[Vector Store Service]
        RA --> |Analyze| LLM[DeepSeek LLM]
    end

    subgraph External Services
        VS --> |Vector Storage| PIN[(Pinecone DB)]
        LLM --> |API Calls| OR[OpenRouter API]
        VS --> |Embeddings| HF[HuggingFace Models]
    end

    subgraph Utils & Config
        CONF[Settings/Config] --> APP
        LOG[Logging] --> APP
        ENV[Environment Variables] --> CONF
    end

    subgraph Storage
        FS[File System] --> |Resume Files| DP
        PIN --> |Vector Data| VS
    end

    %% Data Flow
    UI --> |Upload Resumes| FS
    DP --> |Processed Chunks| VS
    VS --> |Search Results| RA
    LLM --> |Analysis Results| RA
    RA --> |Final Results| UI

    %% Styling
    classDef external fill:#f9f,stroke:#333,stroke-width:2px
    classDef core fill:#bbf,stroke:#333,stroke-width:2px
    classDef frontend fill:#bfb,stroke:#333,stroke-width:2px
    classDef utils fill:#fbb,stroke:#333,stroke-width:2px
    
    class PIN,OR,HF external
    class RA,DP,VS,LLM core
    class UI,APP frontend
    class CONF,LOG,ENV,FS utils

================================================================================
# File: .\About_Project\ProjectExplanation.md
================================================================================

# Resume Analyzer: Complete Technical Documentation

## Table of Contents

1. [Project Overview](#project-overview)
2. [Project Structure](#project-structure)
3. [Core Components](#core-components)
4. [Configuration](#configuration)
5. [Services](#services)
6. [Frontend Implementation](#frontend-implementation)
7. [External Integrations](#external-integrations)
8. [Error Handling & Logging](#error-handling--logging)

## Project Overview

The Resume Analyzer is an AI-powered system that analyzes resumes against job requirements. It uses advanced NLP techniques, vector databases, and machine learning to provide intelligent resume matching and analysis.

### Key Features

- Resume processing and analysis
- Job requirement matching
- Vector-based similarity search
- AI-powered content analysis
- User-friendly web interface

## Project Structure

```
project_root/
â”œâ”€â”€ app.py                 # Main Streamlit application
â”œâ”€â”€ run.py                # Application launcher
â”œâ”€â”€ config/               # Configuration files
â”‚   â”œâ”€â”€ settings.py       # System settings
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ models/              # AI models
â”‚   â”œâ”€â”€ deepseek_llm.py  # LLM integration
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ services/            # Core services
â”‚   â”œâ”€â”€ document_processor.py
â”‚   â”œâ”€â”€ resume_analyzer.py
â”‚   â”œâ”€â”€ vector_store.py
â”‚   â””â”€â”€ __init__.py
â””â”€â”€ utils/              # Utility functions
    â”œâ”€â”€ logging_config.py
    â””â”€â”€ __init__.py
```

## Core Components

### 1. Settings (config/settings.py)

```python
class Settings:
    def __init__(self):
        # API Keys
        self.OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY')
        self.PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')

        # DeepSeek Model Settings
        self.DEEPSEEK_MODEL = os.getenv('DEEPSEEK_MODEL', 'deepseek/deepseek-r1:nitro')
        self.DEEPSEEK_TEMPERATURE = float(os.getenv('DEEPSEEK_TEMPERATURE', '0.5'))

        # Pinecone Settings
        self.PINECONE_INDEX_NAME = os.getenv('PINECONE_INDEX_NAME', 'resume-analysis')
        self.PINECONE_DIMENSION = int(os.getenv('PINECONE_DIMENSION', '768'))

        # Document Processing Settings
        self.CHUNK_SIZE = int(os.getenv('CHUNK_SIZE', '1000'))
        self.CHUNK_OVERLAP = int(os.getenv('CHUNK_OVERLAP', '100'))
```

### 2. Document Processor (services/document_processor.py)

The EnhancedDocumentProcessor handles document parsing and chunking:

```python
class EnhancedDocumentProcessor:
    def create_chunks(self, text: str, metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Creates smart chunks based on document content"""
        # Try header-based splitting first
        try:
            md_chunks = self._try_markdown_splitting(text)
            if md_chunks:
                return self._process_markdown_chunks(md_chunks, metadata)
        except Exception:
            pass

        # Fall back to semantic chunking
        return self._create_semantic_chunks(text, metadata)
```

### 3. Vector Store Service (services/vector_store.py)

Manages vector embeddings and similarity search:

```python
class VectorStoreService:
    def __init__(self, api_key: str):
        self.pc = Pinecone(api_key=api_key)
        self.embeddings = self._setup_embeddings()

    def similarity_search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """Performs similarity search in vector store"""
        query_embedding = self.embeddings.embed_query(query)
        results = self._index.query(
            vector=query_embedding,
            top_k=k,
            include_metadata=True
        )
        return self._format_results(results)
```

### 4. Resume Analyzer (services/resume_analyzer.py)

Core analysis service that coordinates the entire process:

```python
class ResumeAnalyzer:
    async def analyze_resumes(self, job_requirements: str) -> Dict[str, Any]:
        """Analyzes resumes against job requirements"""
        # Get relevant resume chunks
        docs = self.vectorstore.similarity_search(job_requirements, k=10)

        # Group by resume
        resume_contents = self._group_by_resume(docs)

        # Analyze each resume
        analysis_results = []
        for resume_file, contents in resume_contents.items():
            result = await self._analyze_single_resume(
                contents,
                job_requirements
            )
            analysis_results.append(result)

        return {
            "analysis": sorted(analysis_results, key=lambda x: x['match_score'], reverse=True),
            "total_resumes": len(resume_contents)
        }
```

### 5. DeepSeek LLM Integration (models/deepseek_llm.py)

Handles interaction with the AI model:

```python
class OpenRouterDeepSeek:
    async def ainvoke(self, prompt: str) -> str:
        """Asynchronously invokes the DeepSeek model"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.temperature
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(self.api_base, headers=headers, json=payload) as response:
                result = await response.json()
                return result["choices"][0]["message"]["content"]
```

## Frontend Implementation (app.py)

### 1. Main Application Structure

```python
def main():
    st.set_page_config(page_title="Resume Analysis System", layout="wide")
    initialize_session_state()

    # Build UI
    with st.sidebar:
        build_sidebar()
    build_main_content()
```

### 2. File Upload and Processing

```python
def handle_file_upload(uploaded_files):
    """Handles resume file uploads"""
    if uploaded_files:
        try:
            os.makedirs(settings.RESUME_DIR, exist_ok=True)
            for file in uploaded_files:
                save_uploaded_file(file)
            process_uploaded_files()
        except Exception as e:
            handle_upload_error(e)
```

### 3. Analysis Interface

```python
def build_analysis_interface():
    """Builds the analysis interface"""
    st.header("Job Requirements")
    job_requirements = st.text_area(
        "Enter the job requirements:",
        value=st.session_state.job_requirements,
        height=200
    )

    if st.button("Analyze Resumes") and job_requirements:
        run_analysis(job_requirements)
```

## Error Handling & Logging

### 1. Logging Configuration

```python
def setup_logging():
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG)

    # Add handlers
    ch = logging.StreamHandler(sys.stdout)
    fh = logging.FileHandler('app_debug.log')

    # Set formatter
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    ch.setFormatter(formatter)
    fh.setFormatter(formatter)

    logger.addHandler(ch)
    logger.addHandler(fh)
    return logger
```

### 2. Error Handling

```python
def handle_operation_error(operation: str, error: Exception):
    """Centralized error handling"""
    logger.error(f"Error during {operation}: {str(error)}")
    st.error(f"âŒ Error: {str(error)}")

    if isinstance(error, APIError):
        handle_api_error(error)
    elif isinstance(error, FileNotFoundError):
        handle_file_error(error)
    else:
        handle_general_error(error)
```

## External Integrations

### 1. Pinecone Integration

The system uses Pinecone for vector storage and similarity search:

```python
def initialize_pinecone():
    """Initialize Pinecone connection"""
    try:
        pinecone.init(
            api_key=settings.PINECONE_API_KEY,
            environment=settings.PINECONE_ENVIRONMENT
        )

        # Create index if it doesn't exist
        if settings.PINECONE_INDEX_NAME not in pinecone.list_indexes():
            pinecone.create_index(
                name=settings.PINECONE_INDEX_NAME,
                dimension=settings.PINECONE_DIMENSION,
                metric=settings.PINECONE_METRIC
            )

        return pinecone.Index(settings.PINECONE_INDEX_NAME)
    except Exception as e:
        logger.error(f"Pinecone initialization failed: {str(e)}")
        raise
```

### 2. DeepSeek Integration

Integration with the DeepSeek language model via OpenRouter:

```python
async def analyze_resume_content(content: str, requirements: str) -> Dict[str, Any]:
    """Analyze resume content using DeepSeek"""
    prompt = construct_analysis_prompt(content, requirements)

    try:
        response = await deepseek.ainvoke(prompt)
        return parse_llm_response(response)
    except Exception as e:
        logger.error(f"DeepSeek analysis failed: {str(e)}")
        raise
```

## Key Features Implementation

### 1. Resume Processing

```python
def process_resume(file_path: str) -> Dict[str, Any]:
    """Process a single resume file"""
    # Load document
    if file_path.endswith('.pdf'):
        text = extract_pdf_text(file_path)
    else:
        text = extract_text_file(file_path)

    # Create chunks
    chunks = document_processor.create_chunks(text)

    # Extract metadata
    metadata = document_processor.extract_metadata(text, file_path)

    # Create embeddings
    embeddings = vector_store.create_embeddings(chunks)

    return {
        'chunks': chunks,
        'metadata': metadata,
        'embeddings': embeddings
    }
```

### 2. Analysis Pipeline

```python
async def analyze_resume(resume_content: str, job_requirements: str) -> Dict[str, Any]:
    """Complete resume analysis pipeline"""
    # Step 1: Extract relevant information
    relevant_chunks = vector_store.similarity_search(
        job_requirements,
        resume_content
    )

    # Step 2: Analyze with LLM
    analysis = await deepseek.analyze_content(
        relevant_chunks,
        job_requirements
    )

    # Step 3: Process and structure results
    structured_results = process_analysis_results(analysis)

    return structured_results
```

### 3. Results Processing

```python
def process_analysis_results(raw_results: Dict[str, Any]) -> Dict[str, Any]:
    """Process and structure analysis results"""
    return {
        'match_score': calculate_match_score(raw_results),
        'qualifications': extract_qualifications(raw_results),
        'missing_requirements': identify_missing_requirements(raw_results),
        'experience': calculate_experience(raw_results),
        'summary': generate_summary(raw_results)
    }
```

## Usage Example

```python
# Initialize the system
analyzer = ResumeAnalyzer(
    openrouter_api_key=settings.OPENROUTER_API_KEY,
    pinecone_api_key=settings.PINECONE_API_KEY
)

# Process resumes
success, message = analyzer.process_new_resumes()

# Analyze against job requirements
results = await analyzer.analyze_resumes(job_requirements)

# Display results
display_analysis_results(results)
```

This documentation covers the main components and functionality of the Resume Analyzer project. Each component is designed to be modular and maintainable, with clear separation of concerns and robust error handling.


================================================================================
# File: .\About_Project\Streamlit.md
================================================================================

# Resume Analyzer: Complete Streamlit Implementation Guide

## Table of Contents

1. [Project Structure](#project-structure)
2. [Main Application (app.py)](#main-application)
3. [Core Components](#core-components)
4. [Page Layout](#page-layout)
5. [Implementation Details](#implementation-details)
6. [Code Examples](#code-examples)

## Project Structure

```
project_root/
â”‚
â”œâ”€â”€ app.py                # Main Streamlit application
â”œâ”€â”€ run.py               # Streamlit app launcher
â”œâ”€â”€ test.py              # Streamlit test file
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.py      # Configuration affecting Streamlit
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ services/            # Backend services called by Streamlit
â”‚   â”œâ”€â”€ resume_analyzer.py
â”‚   â”œâ”€â”€ vector_store.py
â”‚   â””â”€â”€ document_processor.py
â”‚
â””â”€â”€ utils/
    â””â”€â”€ logging_config.py # Logging used in Streamlit
```

## Main Application

### app.py - Core Structure

```python
import streamlit as st
import os
import sys
import asyncio
import warnings
from typing import Dict, Any, List

def main():
    # Page Configuration
    st.set_page_config(
        page_title="Resume Analysis System",
        page_icon="ðŸ“„",
        layout="wide"
    )

    initialize_session_state()

    # Main UI Components
    st.title("ðŸ“„ Resume Analysis System")

    # Side Bar
    with st.sidebar:
        build_sidebar()

    # Main Content
    build_main_content()

def initialize_session_state():
    """Initialize Streamlit session state variables"""
    if 'resume_analyzer' not in st.session_state:
        st.session_state.resume_analyzer = None
    if 'job_requirements' not in st.session_state:
        st.session_state.job_requirements = ""
    if 'refresh_key' not in st.session_state:
        st.session_state.refresh_key = 0
    if 'processing_status' not in st.session_state:
        st.session_state.processing_status = None

if __name__ == "__main__":
    main()
```

## Core Components

### 1. Session State Management

```python
def initialize_session_state():
    """Initialize all required session state variables"""
    states = {
        'resume_analyzer': None,
        'job_requirements': "",
        'refresh_key': 0,
        'processing_status': None
    }

    for key, default_value in states.items():
        if key not in st.session_state:
            st.session_state[key] = default_value
```

### 2. File Upload Handler

```python
def handle_file_upload(uploaded_files):
    """Process uploaded resume files"""
    if uploaded_files:
        try:
            os.makedirs(settings.RESUME_DIR, exist_ok=True)
            for file in uploaded_files:
                file_path = os.path.join(settings.RESUME_DIR, file.name)
                with open(file_path, "wb") as f:
                    f.write(file.getbuffer())
            st.success(f"âœ… Saved {len(uploaded_files)} resumes")
            return True
        except Exception as e:
            st.error(f"Error saving files: {str(e)}")
            logger.error(f"File upload error: {str(e)}")
            return False
```

### 3. Resume Display

```python
def display_stored_resumes():
    """Display and manage stored resumes in sidebar"""
    if st.session_state.resume_analyzer is None:
        return

    stored_resumes = st.session_state.resume_analyzer.list_stored_resumes()

    if stored_resumes:
        st.sidebar.markdown("---")
        st.sidebar.header("Stored Resumes")
        for resume in stored_resumes:
            col1, col2 = st.sidebar.columns([4, 1])
            with col1:
                st.write(f"ðŸ“„ {os.path.basename(resume)}")
            with col2:
                if st.button("ðŸ—‘ï¸", key=f"del_{resume}"):
                    if st.session_state.resume_analyzer.delete_resume(resume):
                        st.success(f"Deleted {resume}")
                        st.rerun()
```

## Page Layout

### 1. Sidebar Structure

```python
def build_sidebar():
    st.header("System Controls")

    # Initialize System Button
    if st.button("Initialize System", use_container_width=True):
        initialize_system()

    # File Upload Section
    st.header("Resume Upload")
    handle_file_upload_section()

    # Display stored resumes
    display_stored_resumes()
```

### 2. Main Content Structure

```python
def build_main_content():
    # System Status Display
    with st.expander("System Status", expanded=True):
        display_system_status()

    # Job Requirements Section
    st.header("Job Requirements")
    job_requirements = st.text_area(
        "Enter the job requirements:",
        value=st.session_state.job_requirements,
        height=200
    )

    # Analysis Controls
    col1, col2 = st.columns([1, 4])
    with col1:
        analyze_button = st.button("Analyze Resumes", type="primary")

    # Handle Analysis
    if analyze_button and job_requirements:
        handle_resume_analysis(job_requirements)
```

## Implementation Details

### 1. System Status Display

```python
def display_system_status():
    col1, col2 = st.columns(2)
    with col1:
        st.write("API Status:")
        st.write(f"OpenRouter Key: {'âœ…' if settings.OPENROUTER_API_KEY else 'âŒ'}")
        st.write(f"Pinecone Key: {'âœ…' if settings.PINECONE_API_KEY else 'âŒ'}")
    with col2:
        st.write("System Status:")
        st.write(f"System Initialized: {'âœ…' if st.session_state.resume_analyzer else 'âŒ'}")
        if st.session_state.processing_status:
            success, message = st.session_state.processing_status
            st.write(f"Last Process Status: {'âœ…' if success else 'âŒ'} {message}")
```

### 2. Analysis Results Display

```python
def display_analysis_results(results):
    if not results or not results.get('analysis'):
        st.warning("No analysis results to display")
        return

    st.write("### Analysis Results")
    st.write(f"Total Resumes Analyzed: {results['total_resumes']}")

    for result in results['analysis']:
        with st.expander(f"ðŸ“„ {os.path.basename(result['resume_file'])} "
                        f"(Match Score: {result['match_score']}%)"):
            display_result_details(result)

def display_result_details(result):
    st.write("**Key Matching Qualifications:**")
    for qual in result['qualifications_match']:
        st.write(f"- {qual}")

    st.write("\n**Missing Requirements:**")
    for req in result['missing_requirements']:
        st.write(f"- {req}")

    st.write("\n**Additional Relevant Skills:**")
    for skill in result['additional_skills']:
        st.write(f"- {skill}")

    st.write(f"\n**Years of Experience:** {result['years_experience']}")
    st.write(f"\n**Summary:** {result['summary']}")
```

## Code Examples

### 1. Async Operations

```python
async def analyze_resumes(job_requirements):
    with st.spinner("Analyzing resumes..."):
        try:
            results = await st.session_state.resume_analyzer.analyze_resumes(
                job_requirements
            )
            if results:
                st.success("Analysis completed!")
                display_analysis_results(results)
            else:
                st.warning("No results to display")
        except Exception as e:
            st.error(f"âŒ Error analyzing resumes: {str(e)}")
            logger.error(f"Analysis error: {str(e)}")
```

### 2. File Processing

```python
def process_uploaded_resumes():
    with st.spinner("Processing resumes..."):
        try:
            success, message = st.session_state.resume_analyzer.process_new_resumes()
            if success:
                st.success(f"âœ… {message}")
                time.sleep(1)
                st.rerun()
            else:
                st.error(f"âŒ {message}")
        except Exception as e:
            st.error(f"âŒ Error processing resumes: {str(e)}")
```

### Best Practices

1. **State Management**

   - Initialize all session state variables at startup
   - Use session state for data persistence
   - Clear state when appropriate

2. **Error Handling**

   - Wrap operations in try-except blocks
   - Provide clear error messages
   - Use appropriate status indicators

3. **User Experience**

   - Show loading indicators for long operations
   - Provide clear feedback
   - Maintain consistent layout

4. **Performance**
   - Cache expensive computations
   - Handle large files efficiently
   - Use async operations for API calls

### Running the Application

To run the Streamlit application:

```bash
streamlit run app.py
```

This will start the web server and open the application in your default browser. The application will be accessible at `http://localhost:8501` by default.


================================================================================
# File: .\LICENSE
================================================================================

MIT License

Copyright (c) 2025 Shashank Gonchigar

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


================================================================================
# File: .\__init__.py
================================================================================

# resume_analyzer/services/__init__.py
"""Services package for the Resume Analysis System."""

from .document_processor import EnhancedDocumentProcessor
from .vector_store import VectorStoreService
from .resume_analyzer import ResumeAnalyzer

__all__ = ['EnhancedDocumentProcessor', 'VectorStoreService', 'ResumeAnalyzer']

================================================================================
# File: .\app.py
================================================================================

import startup  # Import startup configuration first
import streamlit as st
import asyncio
import nest_asyncio
from typing import Dict, Any
import os
import sys
import warnings
import time
from typing import List, Optional, Union
import logging
import torch

# Apply nest_asyncio to handle nested event loops
nest_asyncio.apply()

# Disable PyTorch warnings and class inspection
import torch._classes
torch._classes.__getattr__ = lambda *args: None

# Import project modules
from config.settings import settings
from services.resume_analyzer import ResumeAnalyzer

def initialize_async_environment():
    """Initialize the async environment properly"""
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    return loop

def initialize_session_state():
    """Initialize Streamlit session state variables"""
    if 'resume_analyzer' not in st.session_state:
        st.session_state.resume_analyzer = None
    if 'job_requirements' not in st.session_state:
        st.session_state.job_requirements = ""
    if 'refresh_key' not in st.session_state:
        st.session_state.refresh_key = 0
    if 'processing_status' not in st.session_state:
        st.session_state.processing_status = None
    if 'async_loop' not in st.session_state:
        st.session_state.async_loop = initialize_async_environment()

async def initialize_resume_analyzer():
    """Asynchronously initialize the resume analyzer"""
    try:
        return ResumeAnalyzer(
            openrouter_api_key=settings.OPENROUTER_API_KEY,
            pinecone_api_key=settings.PINECONE_API_KEY
        )
    except Exception as e:
        st.error(f"Failed to initialize Resume Analyzer: {str(e)}")
        raise

def handle_file_upload(uploaded_files):
    """Handle uploaded resume files"""
    if uploaded_files:
        try:
            os.makedirs(settings.RESUME_DIR, exist_ok=True)
            for file in uploaded_files:
                file_path = os.path.join(settings.RESUME_DIR, file.name)
                with open(file_path, "wb") as f:
                    f.write(file.getbuffer())
            st.success(f"âœ… Saved {len(uploaded_files)} resumes")
            return True
        except Exception as e:
            st.error(f"Error saving files: {str(e)}")
            return False

def display_stored_resumes():
    """Display and manage stored resumes"""
    if st.session_state.resume_analyzer is None:
        return

    stored_resumes = st.session_state.resume_analyzer.list_stored_resumes()
    if stored_resumes:
        st.sidebar.markdown("---")
        st.sidebar.header("Stored Resumes")
        for resume in stored_resumes:
            col1, col2 = st.sidebar.columns([4, 1])
            with col1:
                st.write(f"ðŸ“„ {os.path.basename(resume)}")
            with col2:
                if st.button("ðŸ—‘ï¸", key=f"del_{resume}"):
                    if st.session_state.resume_analyzer.delete_resume(resume):
                        st.success(f"Deleted {resume}")
                        time.sleep(1)
                        st.rerun()

def main():
    # Apply nest_asyncio for nested event loops
    nest_asyncio.apply()
    
    # Initialize session state
    initialize_session_state()
    
    # Configure Streamlit page
    st.set_page_config(
        page_title="Resume Analysis System",
        page_icon="ðŸ“„",
        layout="wide"
    )
    
    st.title("ðŸ“„ Resume Analysis System")
    
    # System Status Display
    with st.expander("System Status", expanded=True):
        col1, col2 = st.columns(2)
        with col1:
            st.write("API Status:")
            st.write(f"OpenRouter Key: {'âœ…' if settings.OPENROUTER_API_KEY else 'âŒ'}")
            st.write(f"Pinecone Key: {'âœ…' if settings.PINECONE_API_KEY else 'âŒ'}")
        with col2:
            st.write("System Status:")
            st.write(f"System Initialized: {'âœ…' if st.session_state.resume_analyzer else 'âŒ'}")
    
    # Sidebar
    with st.sidebar:
        st.header("System Controls")
        
        # Initialize System Button
        if st.button("Initialize System", use_container_width=True):
            with st.spinner("Initializing..."):
                try:
                    loop = st.session_state.async_loop
                    st.session_state.resume_analyzer = loop.run_until_complete(
                        initialize_resume_analyzer()
                    )
                    st.success("âœ… System initialized!")
                    st.rerun()
                except Exception as e:
                    st.error(f"âŒ Error: {str(e)}")
        
        # File Upload Section
        st.header("Resume Upload")
        uploaded_files = st.file_uploader(
            "Upload Resumes (PDF, TXT)", 
            accept_multiple_files=True,
            type=['pdf', 'txt']
        )
        
        if uploaded_files:
            if handle_file_upload(uploaded_files):
                if st.button("Process Uploaded Resumes", use_container_width=True):
                    with st.spinner("Processing resumes..."):
                        try:
                            success, message = st.session_state.resume_analyzer.process_new_resumes()
                            if success:
                                st.success(f"âœ… {message}")
                                time.sleep(1)
                                st.rerun()
                            else:
                                st.error(f"âŒ {message}")
                        except Exception as e:
                            st.error(f"âŒ Error processing resumes: {str(e)}")
        
        # Display stored resumes
        display_stored_resumes()
    
    # Main Content Area
    if st.session_state.resume_analyzer:
        st.header("Job Requirements")
        job_requirements = st.text_area(
            "Enter the job requirements:",
            value=st.session_state.job_requirements,
            height=200
        )
        
        col1, col2 = st.columns([1, 4])
        with col1:
            analyze_button = st.button("Analyze Resumes", type="primary")
        
        if analyze_button and job_requirements:
            st.session_state.job_requirements = job_requirements
            with st.spinner("Analyzing resumes..."):
                try:
                    loop = st.session_state.async_loop
                    results = loop.run_until_complete(
                        st.session_state.resume_analyzer.analyze_resumes(job_requirements)
                    )
                    if results:
                        st.success("Analysis completed!")
                        display_analysis_results(results)
                    else:
                        st.warning("No results to display")
                except Exception as e:
                    st.error(f"âŒ Error analyzing resumes: {str(e)}")
        elif analyze_button:
            st.warning("Please enter job requirements first")
    else:
        st.info("ðŸ‘ˆ Please initialize the system using the sidebar controls")

if __name__ == "__main__":
    main()

================================================================================
# File: .\app_debug.log
================================================================================

2025-02-24 09:31:32,196 - utils.logging_config - INFO - Initializing ResumeAnalyzer
2025-02-24 09:31:49,363 - utils.logging_config - INFO - Using existing index: resume-analysis
2025-02-24 09:31:50,235 - utils.logging_config - INFO - Successfully connected to Pinecone index
2025-02-24 09:32:05,941 - utils.logging_config - INFO - Starting resume analysis...
2025-02-24 09:32:06,884 - utils.logging_config - INFO - Retrieved 6 relevant chunks
2025-02-24 09:32:06,884 - utils.logging_config - INFO - Analyzing 3 resumes
2025-02-24 09:32:06,885 - utils.logging_config - INFO - Analyzing resume: resumes\Shashank.txt
2025-02-24 09:32:07,365 - utils.logging_config - ERROR - Error calling OpenRouter API: 200, message='Attempt to decode JSON with unexpected mimetype: text/html; charset=utf-8', url='https://openrouter.ai/api/v1'
2025-02-24 09:32:07,365 - utils.logging_config - ERROR - Error analyzing resumes\Shashank.txt: 200, message='Attempt to decode JSON with unexpected mimetype: text/html; charset=utf-8', url='https://openrouter.ai/api/v1'
2025-02-24 09:32:07,365 - utils.logging_config - INFO - Analyzing resume: resumes\Alex.txt
2025-02-24 09:32:07,925 - utils.logging_config - ERROR - Error calling OpenRouter API: 200, message='Attempt to decode JSON with unexpected mimetype: text/html; charset=utf-8', url='https://openrouter.ai/api/v1'
2025-02-24 09:32:07,925 - utils.logging_config - ERROR - Error analyzing resumes\Alex.txt: 200, message='Attempt to decode JSON with unexpected mimetype: text/html; charset=utf-8', url='https://openrouter.ai/api/v1'
2025-02-24 09:32:07,926 - utils.logging_config - INFO - Analyzing resume: resumes\David.txt
2025-02-24 09:32:08,473 - utils.logging_config - ERROR - Error calling OpenRouter API: 200, message='Attempt to decode JSON with unexpected mimetype: text/html; charset=utf-8', url='https://openrouter.ai/api/v1'
2025-02-24 09:32:08,474 - utils.logging_config - ERROR - Error analyzing resumes\David.txt: 200, message='Attempt to decode JSON with unexpected mimetype: text/html; charset=utf-8', url='https://openrouter.ai/api/v1'
2025-02-24 09:32:08,474 - utils.logging_config - ERROR - Error analyzing resumes: No resumes could be analyzed successfully
2025-02-24 09:32:08,476 - utils.logging_config - ERROR - Analysis error: No resumes could be analyzed successfully
2025-02-24 09:33:34,728 - utils.logging_config - INFO - Initializing ResumeAnalyzer
2025-02-24 09:33:43,424 - utils.logging_config - INFO - Using existing index: resume-analysis
2025-02-24 09:33:44,254 - utils.logging_config - INFO - Successfully connected to Pinecone index
2025-02-24 09:36:26,912 - utils.logging_config - INFO - Initializing ResumeAnalyzer
2025-02-24 09:36:36,185 - utils.logging_config - INFO - Using existing index: resume-analysis
2025-02-24 09:36:36,589 - utils.logging_config - INFO - Successfully connected to Pinecone index
2025-02-24 09:37:56,141 - utils.logging_config - INFO - Initializing ResumeAnalyzer
2025-02-24 09:38:03,801 - utils.logging_config - INFO - Using existing index: resume-analysis
2025-02-24 09:38:04,598 - utils.logging_config - INFO - Successfully connected to Pinecone index
2025-02-24 09:40:28,986 - utils.logging_config - INFO - Initializing ResumeAnalyzer
2025-02-24 09:40:37,185 - utils.logging_config - INFO - Using existing index: resume-analysis
2025-02-24 09:40:37,741 - utils.logging_config - INFO - Successfully connected to Pinecone index
2025-02-24 09:43:27,838 - utils.logging_config - INFO - Initializing ResumeAnalyzer
2025-02-24 09:43:36,805 - utils.logging_config - INFO - Using existing index: resume-analysis
2025-02-24 09:43:37,855 - utils.logging_config - INFO - Successfully connected to Pinecone index
2025-02-24 09:47:52,645 - utils.logging_config - INFO - Initializing ResumeAnalyzer
2025-02-24 09:47:59,830 - utils.logging_config - INFO - Using existing index: resume-analysis
2025-02-24 09:48:00,774 - utils.logging_config - INFO - Successfully connected to Pinecone index
2025-02-24 09:55:04,276 - utils.logging_config - INFO - Initializing ResumeAnalyzer
2025-02-24 09:55:13,889 - utils.logging_config - INFO - Using existing index: resume-analysis
2025-02-24 09:55:14,600 - utils.logging_config - INFO - Successfully connected to Pinecone index


================================================================================
# File: .\config\__init__.py
================================================================================

# resume_analyzer/config/__init__.py
"""Configuration package for the Resume Analysis System."""

from .settings import *

================================================================================
# File: .\config\settings.py
================================================================================

# resume_analyzer/config/settings.py
import os
from dotenv import load_dotenv

class Settings:
    def __init__(self):
        # Load environment variables
        load_dotenv()
        
        # API Keys
        self.OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY')
        self.PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')
        
        # DeepSeek Model Settings
        self.DEEPSEEK_MODEL = os.getenv('DEEPSEEK_MODEL', 'deepseek/deepseek-r1:nitro')
        self.DEEPSEEK_TEMPERATURE = float(os.getenv('DEEPSEEK_TEMPERATURE', '0.5'))
        self.DEEPSEEK_API_BASE = os.getenv('DEEPSEEK_API_BASE', 'https://openrouter.ai/api/v1')
        
        # Pinecone Settings
        self.PINECONE_INDEX_NAME = os.getenv('PINECONE_INDEX_NAME', 'resume-analysis')
        self.PINECONE_DIMENSION = int(os.getenv('PINECONE_DIMENSION', '768'))
        self.PINECONE_METRIC = os.getenv('PINECONE_METRIC', 'cosine')
        self.PINECONE_CLOUD = os.getenv('PINECONE_CLOUD', 'aws')
        self.PINECONE_REGION = os.getenv('PINECONE_REGION', 'us-east-1')
        
        # Embeddings Settings
        self.EMBEDDINGS_MODEL = os.getenv('EMBEDDINGS_MODEL', 'sentence-transformers/all-mpnet-base-v2')
        self.EMBEDDINGS_DEVICE = os.getenv('EMBEDDINGS_DEVICE', 'cpu').strip()  # Added .strip() to remove any whitespace
        
        # Document Processing Settings
        self.CHUNK_SIZE = int(os.getenv('CHUNK_SIZE', '1000'))
        self.CHUNK_OVERLAP = int(os.getenv('CHUNK_OVERLAP', '100'))
        
        # Data Directory
        self.RESUME_DIR = os.getenv('RESUME_DIR', 'resumes')

# Create a single instance to be imported by other modules
settings = Settings()

# Make settings available for import
__all__ = ['settings']


================================================================================
# File: .\context.md
================================================================================

# Resume Analyzer Project Context

## Overview

The Resume Analyzer project is a Python-based application designed to analyze resumes and provide insights. It includes a web interface component built with JavaScript.

## Key Files and Directories

### app.py

- **Purpose**: Main application file that likely initializes and runs the resume analysis service.
- **Role**: Entry point for the application.

### AnalysisDashboard.js

- **Purpose**: JavaScript file for the web interface, possibly handling the UI logic and data visualization.
- **Role**: Frontend component for displaying analysis results.

### config/

- **Purpose**: Contains configuration settings for the application.
- **Key Files**:
  - `settings.py`: Configuration settings for the application.

### models/

- **Purpose**: Placeholder for machine learning models used in the resume analysis.
- **Key Files**: Currently empty, but will likely contain model files in the future.

### resumes/

- **Purpose**: Contains sample resumes for testing and analysis.
- **Key Files**:
  - `Alex.txt`
  - `David.txt`
  - `Shashank.txt`

### services/

- **Purpose**: Contains service files that handle the core logic of the resume analysis.
- **Key Files**:
  - `document_processor.py`: Processes documents (resumes) for analysis.
  - `resume_analyzer.py`: Analyzes resumes and extracts relevant information.
  - `vector_store.py`: Manages vector storage for resume data.

### utils/

- **Purpose**: Contains utility files that support the application.
- **Key Files**:
  - `logging_config.py`: Configuration for logging within the application.

## Additional Files

- `__init__.py`: Initializes Python packages.
- `.gitignore`: Specifies files and directories to ignore in version control.
- `env_template`: Template for environment variables.
- `LICENSE`: License information for the project.
- `Merge.py`: Script for merging projects.
- `merged_project.txt`: Output of merged projects.
- `projectstructure.md`: Documentation of the project structure.
- `ReadMe.md`: General documentation and instructions for the project.
- `run.py`: Script for running the application.
- `setup.py`: Script for setting up the application.
- `test.py`: Script for running tests.
- `verify.py`: Script for verifying the application.

## Usage

To run the application, execute the `run.py` script. For development, you might need to set up the environment variables using `env_template` and install dependencies using `setup.py`.

## Development

The project is structured to separate concerns, with services handling the core logic, utilities providing support functions, and configuration managing settings. This separation makes the project easier to maintain and extend.

### Initialize System

- **Trigger**: Clicking the "Initialize System" button in the Streamlit interface.
- **Functionality**: Initializes the `ResumeAnalyzer` by creating an instance of it with the required API keys (`OPENROUTER_API_KEY` and `PINECONE_API_KEY`) and stores it in the Streamlit session state.
- **Purpose**: Prepares the system for resume analysis by setting up the necessary services and configurations.


================================================================================
# File: .\env_template
================================================================================

# Required API Keys
# Get your OpenRouter API key from https://openrouter.ai/
OPENROUTER_API_KEY=your_openrouter_api_key_here

# Get your Pinecone API key from https://www.pinecone.io/
PINECONE_API_KEY=your_pinecone_api_key_here

# Optional: Pinecone Settings
# Only change these if you want to use different values from the defaults
PINECONE_INDEX_NAME=resume-analysis
PINECONE_DIMENSION=768
PINECONE_METRIC=cosine
PINECONE_CLOUD=aws
PINECONE_REGION=us-east-1

# Optional: DeepSeek Model Settings
# Only change these if you want to use different model parameters
DEEPSEEK_MODEL=deepseek/deepseek-r1:nitro
DEEPSEEK_TEMPERATURE=0.5
DEEPSEEK_API_BASE=https://openrouter.ai/api/v1

# Optional: Document Processing Settings
# Adjust these based on your needs for text chunking
CHUNK_SIZE=1000
CHUNK_OVERLAP=100

# Optional: Data Directory
# Change this if you want to store resumes in a different directory
RESUME_DIR=resumes

# Optional: Logging Level
# Values: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Optional: Embeddings Settings
# Change these if you want to use a different embeddings model
EMBEDDINGS_MODEL=sentence-transformers/all-mpnet-base-v2
EMBEDDINGS_DEVICE=cpu

================================================================================
# File: .\merge_project.py
================================================================================

import os
import datetime

def merge_python_files(root_dir: str, output_file: str) -> None:
    """
    Merge all Python files in the project into a single file.
    
    Args:
        root_dir: Root directory of the project
        output_file: Output file path
    """
    
    # Header template for the merged file
    header = f'''"""
Resume Analyzer - Complete Project
Generated on: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
This file contains all merged Python code from the Resume Analyzer project.
"""

# ====================== Import Statements ======================
import os
import sys
import asyncio
import streamlit as st
import warnings
import time
from typing import Dict, Any, List, Optional, Union
import logging
import json
from pathlib import Path
from dotenv import load_dotenv
import pinecone
from langchain_pinecone import PineconeVectorStore
from sentence_transformers import SentenceTransformer
import torch
import aiohttp

'''

    # Order of directories to process (to handle dependencies)
    dir_order = [
        'config',
        'utils',
        'models',
        'services',
        ''  # Root directory
    ]
    
    with open(output_file, 'w', encoding='utf-8') as outfile:
        # Write header
        outfile.write(header + '\n')
        
        # Process each directory in order
        for directory in dir_order:
            current_dir = os.path.join(root_dir, directory)
            if not os.path.exists(current_dir):
                continue
                
            # Write section header
            outfile.write(f'\n# ====================== {directory.upper() if directory else "ROOT"} ======================\n\n')
            
            # Get all Python files in current directory
            python_files = [f for f in os.listdir(current_dir) if f.endswith('.py') and f != 'merge_project.py']
            
            # Process each Python file
            for py_file in python_files:
                file_path = os.path.join(current_dir, py_file)
                
                # Write file header
                outfile.write(f'# File: {os.path.join(directory, py_file)}\n')
                
                # Read and write file content
                with open(file_path, 'r', encoding='utf-8') as infile:
                    content = infile.read()
                    
                    # Remove existing imports
                    lines = content.split('\n')
                    filtered_lines = []
                    for line in lines:
                        if not (line.startswith('import ') or line.startswith('from ')):
                            filtered_lines.append(line)
                    
                    outfile.write('\n'.join(filtered_lines) + '\n\n')

if __name__ == "__main__":
    # Project root directory
    project_root = os.path.dirname(os.path.abspath(__file__))
    
    # Output file path
    output_path = os.path.join(project_root, 'resume_analyzer_complete.py')
    
    try:
        merge_python_files(project_root, output_path)
        print(f"Successfully merged all Python files into: {output_path}")
    except Exception as e:
        print(f"Error merging files: {str(e)}")

================================================================================
# File: .\models\__init__.py
================================================================================

# models/__init__.py
"""Models package for the Resume Analysis System."""

from .deepseek_llm import OpenRouterDeepSeek

__all__ = ['OpenRouterDeepSeek']

================================================================================
# File: .\models\deepseek_llm.py
================================================================================

# models/deepseek_llm.py
import json
import aiohttp
from typing import Any, Dict, Optional
from config.settings import settings
from utils.logging_config import logger

class OpenRouterDeepSeek:
    """Class for interacting with OpenRouter's DeepSeek model"""
    
    def __init__(self, api_key: str):
        """Initialize with OpenRouter API key"""
        self.api_key = api_key
        self.api_base = settings.DEEPSEEK_API_BASE
        self.model = settings.DEEPSEEK_MODEL
        self.temperature = settings.DEEPSEEK_TEMPERATURE

    async def ainvoke(self, prompt: str) -> str:
        """
        Asynchronously invoke the DeepSeek model with a prompt
        
        Args:
            prompt (str): The prompt to send to the model
            
        Returns:
            str: The model's response text
            
        Raises:
            Exception: If the API call fails
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://resume-analyzer.example.com",  # Replace with your domain
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.temperature
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.api_base,
                    headers=headers,
                    json=payload
                ) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        raise Exception(f"API request failed: {error_text}")
                    
                    result = await response.json()
                    return result["choices"][0]["message"]["content"]
                    
        except Exception as e:
            logger.error(f"Error calling OpenRouter API: {str(e)}")
            raise

================================================================================
# File: .\models\setup.py
================================================================================

from setuptools import setup, find_packages

setup(
    name="resume_analyzer",
    version="1.0.0",
    packages=find_packages(),
    install_requires=[
        "streamlit",
        "python-dotenv",
        "langchain",
        "langchain-community",
        "langchain-core",
        "pinecone",  # Updated from pinecone-client
        "sentence-transformers",
        "transformers",
        "torch",
        "pypdf",
        "huggingface-hub",
        "nltk",
        "unstructured",  # For document loading
        "markdown",      # For markdown processing
        "aiohttp",      # For async HTTP requests
    ],
)

================================================================================
# File: .\projectstructure.md
================================================================================

resume_analyzer/
â”‚
â”œâ”€â”€ **init**.py
â”œâ”€â”€ config/
â”‚ â”œâ”€â”€ **init**.py
â”‚ â””â”€â”€ settings.py
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ **init**.py
â”‚ â””â”€â”€ deepseek_llm.py
â”œâ”€â”€ services/
â”‚ â”œâ”€â”€ **init**.py
â”‚ â”œâ”€â”€ resume_analyzer.py
â”‚ â””â”€â”€ vector_store.py
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ **init**.py
â”‚ â””â”€â”€ logging_config.py
â””â”€â”€ app.py


================================================================================
# File: .\resume_analyzer.egg-info\PKG-INFO
================================================================================

Metadata-Version: 2.2
Name: resume_analyzer
Version: 1.0.0
Requires-Python: >=3.9
License-File: LICENSE
Requires-Dist: streamlit>=1.29.0
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: langchain>=0.1.0
Requires-Dist: langchain-community>=0.0.10
Requires-Dist: langchain-core>=0.1.10
Requires-Dist: langchain-pinecone>=0.0.2
Requires-Dist: langchain-huggingface>=0.0.2
Requires-Dist: pinecone-client>=3.0.0
Requires-Dist: sentence-transformers>=2.2.2
Requires-Dist: transformers>=4.36.0
Requires-Dist: torch>=2.2.1
Requires-Dist: torchvision>=0.17.1
Requires-Dist: torchaudio>=2.2.1
Requires-Dist: pypdf>=3.17.0
Requires-Dist: huggingface-hub>=0.20.0
Requires-Dist: nltk>=3.8.1
Requires-Dist: unstructured>=0.11.0
Requires-Dist: markdown>=3.5.0
Requires-Dist: aiohttp>=3.9.0
Requires-Dist: nest-asyncio>=1.5.8
Requires-Dist: asyncio>=3.4.3
Requires-Dist: packaging>=23.2
Requires-Dist: typing-extensions>=4.9.0
Dynamic: requires-dist
Dynamic: requires-python


================================================================================
# File: .\resume_analyzer.egg-info\SOURCES.txt
================================================================================

LICENSE
setup.py
config/__init__.py
config/settings.py
models/__init__.py
models/deepseek_llm.py
models/setup.py
resume_analyzer.egg-info/PKG-INFO
resume_analyzer.egg-info/SOURCES.txt
resume_analyzer.egg-info/dependency_links.txt
resume_analyzer.egg-info/requires.txt
resume_analyzer.egg-info/top_level.txt
services/__init__.py
services/document_processor.py
services/resume_analyzer.py
services/vector_store.py
utils/__init__.py
utils/async_utils.py
utils/logging_config.py

================================================================================
# File: .\resume_analyzer.egg-info\dependency_links.txt
================================================================================




================================================================================
# File: .\resume_analyzer.egg-info\requires.txt
================================================================================

streamlit>=1.29.0
python-dotenv>=1.0.0
langchain>=0.1.0
langchain-community>=0.0.10
langchain-core>=0.1.10
langchain-pinecone>=0.0.2
langchain-huggingface>=0.0.2
pinecone-client>=3.0.0
sentence-transformers>=2.2.2
transformers>=4.36.0
torch>=2.2.1
torchvision>=0.17.1
torchaudio>=2.2.1
pypdf>=3.17.0
huggingface-hub>=0.20.0
nltk>=3.8.1
unstructured>=0.11.0
markdown>=3.5.0
aiohttp>=3.9.0
nest-asyncio>=1.5.8
asyncio>=3.4.3
packaging>=23.2
typing-extensions>=4.9.0


================================================================================
# File: .\resume_analyzer.egg-info\top_level.txt
================================================================================

config
models
services
utils


================================================================================
# File: .\resume_analyzer_complete.py
================================================================================

"""
Resume Analyzer - Complete Project
Generated on: 2025-02-24 09:16:58
This file contains all merged Python code from the Resume Analyzer project.
"""

# ====================== Import Statements ======================
import os
import sys
import asyncio
import streamlit as st
import warnings
import time
from typing import Dict, Any, List, Optional, Union
import logging
import json
from pathlib import Path
from dotenv import load_dotenv
import pinecone
from langchain_pinecone import PineconeVectorStore
from sentence_transformers import SentenceTransformer
import torch
import aiohttp



# ====================== CONFIG ======================

# File: config\settings.py
# resume_analyzer/config/settings.py

class Settings:
    def __init__(self):
        # Load environment variables
        load_dotenv()
        
        # API Keys
        self.OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY')
        self.PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')
        
        # DeepSeek Model Settings
        self.DEEPSEEK_MODEL = os.getenv('DEEPSEEK_MODEL', 'deepseek/deepseek-r1:nitro')
        self.DEEPSEEK_TEMPERATURE = float(os.getenv('DEEPSEEK_TEMPERATURE', '0.5'))
        self.DEEPSEEK_API_BASE = os.getenv('DEEPSEEK_API_BASE', 'https://openrouter.ai/api/v1')
        
        # Pinecone Settings
        self.PINECONE_INDEX_NAME = os.getenv('PINECONE_INDEX_NAME', 'resume-analysis')
        self.PINECONE_DIMENSION = int(os.getenv('PINECONE_DIMENSION', '768'))
        self.PINECONE_METRIC = os.getenv('PINECONE_METRIC', 'cosine')
        self.PINECONE_CLOUD = os.getenv('PINECONE_CLOUD', 'aws')
        self.PINECONE_REGION = os.getenv('PINECONE_REGION', 'us-east-1')
        
        # Embeddings Settings
        self.EMBEDDINGS_MODEL = os.getenv('EMBEDDINGS_MODEL', 'sentence-transformers/all-mpnet-base-v2')
        self.EMBEDDINGS_DEVICE = os.getenv('EMBEDDINGS_DEVICE', 'cpu').strip()  # Added .strip() to remove any whitespace
        
        # Document Processing Settings
        self.CHUNK_SIZE = int(os.getenv('CHUNK_SIZE', '1000'))
        self.CHUNK_OVERLAP = int(os.getenv('CHUNK_OVERLAP', '100'))
        
        # Data Directory
        self.RESUME_DIR = os.getenv('RESUME_DIR', 'resumes')

# Create a single instance to be imported by other modules
settings = Settings()

# Make settings available for import
__all__ = ['settings']


# File: config\__init__.py
# resume_analyzer/config/__init__.py
"""Configuration package for the Resume Analysis System."""



# ====================== UTILS ======================

# File: utils\logging_config.py

def setup_logging():
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG)
    
    # Console handler
    ch = logging.StreamHandler(sys.stdout)
    ch.setLevel(logging.DEBUG)
    
    # File handler
    fh = logging.FileHandler('app_debug.log')
    fh.setLevel(logging.DEBUG)

    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    ch.setFormatter(formatter)
    fh.setFormatter(formatter)

    logger.addHandler(ch)
    logger.addHandler(fh)
    return logger

logger = setup_logging()


# File: utils\__init__.py
# resume_analyzer/utils/__init__.py
"""Utilities package for the Resume Analysis System."""



# ====================== MODELS ======================

# File: models\deepseek_llm.py
# models/deepseek_llm.py

class OpenRouterDeepSeek:
    """Class for interacting with OpenRouter's DeepSeek model"""
    
    def __init__(self, api_key: str):
        """Initialize with OpenRouter API key"""
        self.api_key = api_key
        self.api_base = settings.DEEPSEEK_API_BASE
        self.model = settings.DEEPSEEK_MODEL
        self.temperature = settings.DEEPSEEK_TEMPERATURE

    async def ainvoke(self, prompt: str) -> str:
        """
        Asynchronously invoke the DeepSeek model with a prompt
        
        Args:
            prompt (str): The prompt to send to the model
            
        Returns:
            str: The model's response text
            
        Raises:
            Exception: If the API call fails
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://resume-analyzer.example.com",  # Replace with your domain
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.temperature
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.api_base,
                    headers=headers,
                    json=payload
                ) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        raise Exception(f"API request failed: {error_text}")
                    
                    result = await response.json()
                    return result["choices"][0]["message"]["content"]
                    
        except Exception as e:
            logger.error(f"Error calling OpenRouter API: {str(e)}")
            raise

# File: models\setup.py

setup(
    name="resume_analyzer",
    version="1.0.0",
    packages=find_packages(),
    install_requires=[
        "streamlit",
        "python-dotenv",
        "langchain",
        "langchain-community",
        "langchain-core",
        "pinecone",  # Updated from pinecone-client
        "sentence-transformers",
        "transformers",
        "torch",
        "pypdf",
        "huggingface-hub",
        "nltk",
        "unstructured",  # For document loading
        "markdown",      # For markdown processing
        "aiohttp",      # For async HTTP requests
    ],
)

# File: models\__init__.py
# models/__init__.py
"""Models package for the Resume Analysis System."""


__all__ = ['OpenRouterDeepSeek']


# ====================== SERVICES ======================

# File: services\document_processor.py

class EnhancedDocumentProcessor:
    """Enhanced document processing with smart chunking strategies"""
    
    def __init__(self):
        # Download required NLTK data
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            nltk.download('punkt')
    
    def create_chunks(self, text: str, metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Create smart chunks based on document content"""
        # First try header-based splitting for structured documents
        headers_to_split_on = [
            ("#", "Header 1"),
            ("##", "Header 2"),
            ("###", "Header 3"),
        ]
        
        markdown_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=headers_to_split_on
        )
        
        try:
            md_chunks = markdown_splitter.split_text(text)
            if len(md_chunks) > 1:  # If we found headers
                return self._process_markdown_chunks(md_chunks, metadata)
        except Exception as e:
            logger.warning(f"Markdown splitting failed: {str(e)}")
        
        # Fall back to sentence-based splitting
        return self._create_semantic_chunks(text, metadata)
    
    def _process_markdown_chunks(
        self, 
        md_chunks: List[Any], 
        metadata: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Process markdown chunks with headers"""
        processed_chunks = []
        
        for chunk in md_chunks:
            chunk_metadata = metadata.copy()
            # Add header information to metadata
            for header_level, header_text in chunk.metadata.items():
                chunk_metadata[f"header_{header_level}"] = header_text
            
            processed_chunks.append({
                "text": chunk.page_content,
                "metadata": chunk_metadata
            })
        
        return processed_chunks
    
    def _create_semantic_chunks(
        self, 
        text: str, 
        metadata: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Create chunks based on semantic boundaries"""
        # First split into sentences
        sentences = sent_tokenize(text)
        
        # Group sentences into chunks
        chunks = []
        current_chunk = []
        current_length = 0
        
        for sentence in sentences:
            sentence_length = len(sentence)
            
            if current_length + sentence_length > settings.CHUNK_SIZE:
                if current_chunk:  # Save current chunk if it exists
                    chunks.append({
                        "text": " ".join(current_chunk),
                        "metadata": metadata.copy()
                    })
                current_chunk = [sentence]
                current_length = sentence_length
            else:
                current_chunk.append(sentence)
                current_length += sentence_length
        
        # Add the last chunk if it exists
        if current_chunk:
            chunks.append({
                "text": " ".join(current_chunk),
                "metadata": metadata.copy()
            })
        
        return chunks
    
    def extract_metadata(self, text: str, filename: str) -> Dict[str, Any]:
        """Extract relevant metadata from document content"""
        metadata = {
            "source": filename,
            "char_count": len(text),
            "estimated_read_time": len(text.split()) / 200  # Assuming 200 wpm reading speed
        }
        
        # Try to identify document sections
        sections = []
        current_section = ""
        for line in text.split('\n'):
            if line.strip().startswith('#'):
                if current_section:
                    sections.append(current_section)
                current_section = line.strip('#').strip()
        if current_section:
            sections.append(current_section)
        
        if sections:
            metadata["sections"] = sections
        
        return metadata

# File: services\resume_analyzer.py
# resume_analyzer/services/resume_analyzer.py


class ResumeAnalyzer:
    def __init__(self, openrouter_api_key: str, pinecone_api_key: str):
        """Initialize the Resume Analyzer with required API keys"""
        logger.info("Initializing ResumeAnalyzer")
        self.openrouter_api_key = openrouter_api_key
        self.vector_store = VectorStoreService(pinecone_api_key)
        self.llm = OpenRouterDeepSeek(api_key=openrouter_api_key)
        
        # Initialize vector store and process existing resumes
        try:
            self.vectorstore = self._initialize_vectorstore()
        except Exception as e:
            logger.error(f"Error initializing vector store: {str(e)}")
            raise

    def _initialize_vectorstore(self):
        """Initialize vector store and process existing resumes"""
        # Initialize Pinecone connection
        index = self.vector_store.initialize_store()
        
        # Create VectorStore instance
        if index:
            return PineconeVectorStore(
                index=index,
                embedding=self.vector_store.embeddings,
                text_key="text"
            )
        return None

    def process_new_resumes(self, data_dir: str = settings.RESUME_DIR) -> Tuple[bool, str]:
        """Process only newly added resumes"""
        try:
            logger.info(f"Processing new resumes from directory: {data_dir}")
            
            if not os.path.exists(data_dir):
                logger.error(f"Directory not found: {data_dir}")
                return False, "Resume directory not found"
            
            # Get list of files and currently stored resumes
            files = [f for f in os.listdir(data_dir) 
                    if os.path.isfile(os.path.join(data_dir, f))]
            stored_resumes = self.vector_store.list_documents()
            
            # Filter for new files
            new_files = [f for f in files 
                        if os.path.join(data_dir, f) not in stored_resumes]
            
            if not new_files:
                logger.info("No new resumes to process")
                return True, "No new resumes to process"
            
            logger.info(f"Found {len(new_files)} new files to process")
            
            # Process new files
            documents = []
            for file in new_files:
                file_path = os.path.join(data_dir, file)
                try:
                    if file.lower().endswith('.pdf'):
                        loader = PyPDFLoader(file_path)
                    else:
                        loader = TextLoader(file_path)
                    documents.extend(loader.load())
                except Exception as e:
                    logger.error(f"Error loading {file}: {str(e)}")
            
            if not documents:
                return False, "No new resumes could be processed"
            
            # Create chunks and update vector store
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=settings.CHUNK_SIZE,
                chunk_overlap=settings.CHUNK_OVERLAP,
                length_function=len,
                add_start_index=True,
            )
            chunks = text_splitter.split_documents(documents)
            
            # Update vector store
            self.vectorstore = self.vector_store.create_from_documents(chunks)
            
            return True, f"Successfully processed {len(new_files)} new resumes"
            
        except Exception as e:
            logger.error(f"Error processing new resumes: {str(e)}")
            return False, f"Error processing new resumes: {str(e)}"

    def list_stored_resumes(self) -> List[str]:
        """Return list of resumes stored in the vector database"""
        return self.vector_store.list_documents()
    
    def delete_resume(self, resume_filename: str) -> bool:
        """Delete a specific resume from the vector database"""
        success = self.vector_store.delete_document(resume_filename)
        if success:
            # Reinitialize vector store after deletion
            try:
                self.vectorstore = self._initialize_vectorstore()
            except Exception as e:
                logger.error(f"Error reinitializing vector store after deletion: {str(e)}")
        return success
    async def analyze_resumes(self, job_requirements: str) -> Dict[str, Any]:
        """
        Analyze resumes against job requirements
        
        Args:
            job_requirements: String containing job requirements
            
        Returns:
            Dictionary containing analysis results and metadata
            
        Raises:
            ValueError: If vector store is not initialized
            Exception: If analysis fails
        """
        try:
            if not self.vectorstore:
                logger.error("Vector store not initialized")
                raise ValueError("Vector store not initialized. Please process resumes first.")

            logger.info("Starting resume analysis...")
            # Retrieve relevant resume chunks
            docs = self.vectorstore.similarity_search(job_requirements, k=10)
            logger.info(f"Retrieved {len(docs)} relevant chunks")
            
            # Group chunks by resume
            resume_contents = {}
            for doc in docs:
                source = doc.metadata.get('source', 'Unknown')
                if source not in resume_contents:
                    resume_contents[source] = []
                resume_contents[source].append(doc.page_content)
            
            logger.info(f"Analyzing {len(resume_contents)} resumes")
            
            # Analyze each resume
            analysis_results = []
            for resume_file, contents in resume_contents.items():
                logger.info(f"Analyzing resume: {resume_file}")
                full_content = "\n".join(contents)
                
                analysis_prompt = f"""
                Act as an expert HR analyst. Analyze this resume against the following job requirements. 
                Provide a structured analysis in valid JSON format.
                
                Job Requirements:
                {job_requirements}
                
                Resume Content:
                {full_content}
                
                Response Instructions:
                1. Match Score: Provide a number between 0-100
                2. List 3-5 key matching qualifications
                3. List 2-3 missing requirements
                4. List 2-3 additional relevant skills
                5. Calculate years of relevant experience
                6. Write a 2-3 sentence summary
                
                Required JSON Structure:
                {{
                    "match_score": <number>,
                    "qualifications_match": ["qual1", "qual2", "qual3"],
                    "missing_requirements": ["req1", "req2"],
                    "additional_skills": ["skill1", "skill2"],
                    "years_experience": <number>,
                    "summary": "Brief summary text"
                }}
                
                The response must be ONLY valid JSON with no additional text.
                """
                
                try:
                    result = await self.llm.ainvoke(analysis_prompt)
                    logger.debug(f"Raw LLM response: {result}")
                    
                    # Clean and parse the response
                    result_text = result.strip()
                    if not result_text.startswith('{'):
                        result_text = result_text[result_text.find('{'):]
                    if not result_text.endswith('}'):
                        result_text = result_text[:result_text.rfind('}')+1]
                    
                    parsed_result = json.loads(result_text)
                    
                    # Validate required fields
                    required_fields = [
                        'match_score', 
                        'qualifications_match', 
                        'missing_requirements',
                        'additional_skills', 
                        'years_experience', 
                        'summary'
                    ]
                    
                    for field in required_fields:
                        if field not in parsed_result:
                            raise ValueError(f"Missing required field: {field}")
                    
                    parsed_result["resume_file"] = resume_file
                    analysis_results.append(parsed_result)
                    logger.info(f"Successfully analyzed resume: {resume_file}")
                    
                except Exception as e:
                    logger.error(f"Error analyzing {resume_file}: {str(e)}")
                    continue
            
            if not analysis_results:
                raise Exception("No resumes could be analyzed successfully")
            
            # Sort results by match score
            sorted_results = sorted(
                analysis_results, 
                key=lambda x: x.get('match_score', 0), 
                reverse=True
            )
            
            logger.info("Analysis completed successfully")
            return {
                "analysis": sorted_results,
                "total_resumes": len(resume_contents)
            }
            
        except Exception as e:
            logger.error(f"Error analyzing resumes: {str(e)}")
            raise

# File: services\vector_store.py
# services/vector_store.py


class VectorStoreService:
    """Service for managing vector store operations using Pinecone"""
    
    def __init__(self, api_key: str):
        """
        Initialize the VectorStoreService.
        
        Args:
            api_key (str): Pinecone API key
        """
        self.api_key = api_key
        self.pc = Pinecone(api_key=api_key)
        self.index_name = settings.PINECONE_INDEX_NAME
        self.embeddings = self._setup_embeddings()
        self.document_processor = EnhancedDocumentProcessor()
        self._index = None

    def _setup_embeddings(self) -> HuggingFaceEmbeddings:
        """
        Set up the HuggingFace embeddings model.
        
        Returns:
            HuggingFaceEmbeddings: Configured embeddings model
        """
        try:
            return HuggingFaceEmbeddings(
                model_name=settings.EMBEDDINGS_MODEL,
                model_kwargs={'device': settings.EMBEDDINGS_DEVICE}
            )
        except Exception as e:
            logger.error(f"Error setting up embeddings: {str(e)}")
            raise

    def initialize_store(self) -> Optional[Any]:
        """
        Initialize or connect to Pinecone index.
        
        Returns:
            Optional[Any]: Initialized Pinecone index or None if initialization fails
        """
        try:
            existing_indexes = self.pc.list_indexes()
            
            if self.index_name not in existing_indexes.names():
                logger.info(f"Creating new index: {self.index_name}")
                self.pc.create_index(
                    name=self.index_name,
                    dimension=settings.PINECONE_DIMENSION,
                    metric=settings.PINECONE_METRIC,
                    spec=ServerlessSpec(
                        cloud=settings.PINECONE_CLOUD,
                        region=settings.PINECONE_REGION
                    )
                )
                # Wait for index to be ready
                while self.index_name not in self.pc.list_indexes().names():
                    time.sleep(1)
            else:
                logger.info(f"Using existing index: {self.index_name}")
            
            self._index = self.pc.Index(self.index_name)
            if self._index:
                logger.info("Successfully connected to Pinecone index")
                return self._index
            return None
            
        except Exception as e:
            logger.error(f"Pinecone initialization failed: {str(e)}")
            raise

    def list_documents(self) -> List[str]:
        """
        List all unique document sources stored in the vector database.
        
        Returns:
            List[str]: List of document source paths
        """
        try:
            if not self._index:
                self._index = self.pc.Index(self.index_name)
            
            # Query for all vectors and fetch metadata
            query_response = self._index.query(
                vector=[0] * settings.PINECONE_DIMENSION,  # Dummy vector for metadata query
                top_k=10000,  # Adjust based on your needs
                include_metadata=True
            )
            
            # Extract unique source files from metadata
            unique_sources = set()
            for match in query_response['matches']:
                if 'metadata' in match and 'source' in match['metadata']:
                    unique_sources.add(match['metadata']['source'])
            
            return sorted(list(unique_sources))
            
        except Exception as e:
            logger.error(f"Error listing documents: {str(e)}")
            return []

    def delete_document(self, document_path: str) -> bool:
        """
        Delete all vectors associated with a specific document.
        
        Args:
            document_path (str): Path of the document to delete
            
        Returns:
            bool: True if deletion was successful, False otherwise
        """
        try:
            if not self._index:
                self._index = self.pc.Index(self.index_name)

            logger.info(f"Attempting to delete document: {document_path}")
            
            # Find all vectors associated with this document
            query_response = self._index.query(
                vector=[0] * settings.PINECONE_DIMENSION,
                top_k=10000,
                include_metadata=True
            )
            
            # Get IDs of vectors to delete
            ids_to_delete = [
                match['id'] for match in query_response['matches']
                if match.get('metadata', {}).get('source') == document_path
            ]
            
            if ids_to_delete:
                # Delete vectors in batches
                batch_size = 100
                for i in range(0, len(ids_to_delete), batch_size):
                    batch = ids_to_delete[i:i + batch_size]
                    self._index.delete(ids=batch)
                
                logger.info(f"Successfully deleted {len(ids_to_delete)} vectors for {document_path}")
                return True
            else:
                logger.warning(f"No vectors found for document: {document_path}")
                return False
            
        except Exception as e:
            logger.error(f"Error deleting document {document_path}: {str(e)}")
            return False

    def create_from_documents(self, documents: List[Any]) -> Optional[PineconeVectorStore]:
        """
        Create or update vector store from documents with enhanced processing.
        
        Args:
            documents: List of document objects to process
            
        Returns:
            Optional[PineconeVectorStore]: Initialized vector store or None if creation fails
        """
        try:
            logger.info("Starting document processing...")
            processed_chunks = []
            
            for doc in documents:
                try:
                    source = doc.metadata.get('source', 'Unknown')
                    logger.info(f"Processing document: {source}")
                    
                    # Create chunks with metadata
                    chunk = {
                        "text": doc.page_content,
                        "metadata": {
                            "source": source,
                            "start_index": doc.metadata.get('start_index', 0),
                            "chunk_type": "document",
                            "processing_date": time.strftime("%Y-%m-%d %H:%M:%S")
                        }
                    }
                    processed_chunks.append(chunk)
                    
                except Exception as e:
                    logger.error(f"Error processing document {source}: {str(e)}")
                    continue
            
            if not processed_chunks:
                raise ValueError("No documents were successfully processed")
            
            logger.info(f"Successfully processed {len(processed_chunks)} chunks")
            
            # Create embeddings
            logger.info("Creating embeddings...")
            texts = [chunk["text"] for chunk in processed_chunks]
            metadatas = [chunk["metadata"] for chunk in processed_chunks]
            
            embeddings = self.embeddings.embed_documents(texts)
            logger.info(f"Created {len(embeddings)} embeddings")
            
            # Initialize Pinecone
            index = self.initialize_store()
            if not index:
                raise ValueError("Failed to initialize Pinecone index")
            
            # Batch upload to Pinecone
            logger.info("Uploading to Pinecone...")
            batch_size = 100
            total_uploaded = 0
            
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                batch_embeddings = embeddings[i:i + batch_size]
                batch_metadata = metadatas[i:i + batch_size]
                
                vectors = [
                    (f"vec_{total_uploaded + j}", emb, {"text": text, **meta})
                    for j, (emb, text, meta) in enumerate(zip(batch_embeddings, batch_texts, batch_metadata))
                ]
                
                index.upsert(vectors=vectors)
                total_uploaded += len(batch_texts)
                logger.info(f"Uploaded batch: {total_uploaded}/{len(texts)} vectors")
            
            logger.info("Successfully created vector store")
            return PineconeVectorStore(
                index=index,
                embedding=self.embeddings,
                text_key="text"
            )
            
        except Exception as e:
            logger.error(f"Error creating vector store: {str(e)}")
            raise

    def similarity_search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """
        Perform similarity search in the vector store.
        
        Args:
            query (str): Query text to search for
            k (int): Number of results to return
            
        Returns:
            List[Dict[str, Any]]: List of similar documents with metadata
        """
        try:
            if not self._index:
                raise ValueError("Vector store not initialized")
            
            # Generate query embedding
            query_embedding = self.embeddings.embed_query(query)
            
            # Perform similarity search
            results = self._index.query(
                vector=query_embedding,
                top_k=k,
                include_metadata=True
            )
            
            # Format results
            formatted_results = []
            for match in results['matches']:
                formatted_results.append({
                    'text': match['metadata'].get('text', ''),
                    'score': match['score'],
                    'metadata': {
                        k: v for k, v in match['metadata'].items()
                        if k != 'text'
                    }
                })
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"Error performing similarity search: {str(e)}")
            raise

# File: services\__init__.py
# resume_analyzer/services/__init__.py
"""Services package for the Resume Analysis System."""


__all__ = ['ResumeAnalyzer', 'VectorStoreService', 'EnhancedDocumentProcessor']


# ====================== ROOT ======================

# File: app.py

# Add the project root directory to Python path
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Now use absolute imports

# Suppress warnings
warnings.filterwarnings('ignore')

def initialize_session_state():
    """Initialize Streamlit session state variables"""
    if 'resume_analyzer' not in st.session_state:
        st.session_state.resume_analyzer = None
    if 'job_requirements' not in st.session_state:
        st.session_state.job_requirements = ""
    if 'refresh_key' not in st.session_state:
        st.session_state.refresh_key = 0
    if 'processing_status' not in st.session_state:
        st.session_state.processing_status = None

def handle_file_upload(uploaded_files):
    """Handle uploaded resume files"""
    if uploaded_files:
        try:
            os.makedirs(settings.RESUME_DIR, exist_ok=True)
            for file in uploaded_files:
                file_path = os.path.join(settings.RESUME_DIR, file.name)
                with open(file_path, "wb") as f:
                    f.write(file.getbuffer())
            st.success(f"âœ… Saved {len(uploaded_files)} resumes")
            return True
        except Exception as e:
            st.error(f"Error saving files: {str(e)}")
            logger.error(f"File upload error: {str(e)}")
            return False

def display_stored_resumes():
    """Display and manage stored resumes"""
    if st.session_state.resume_analyzer is None:
        return

    stored_resumes = st.session_state.resume_analyzer.list_stored_resumes()

    if stored_resumes:
        st.sidebar.markdown("---")
        st.sidebar.header("Stored Resumes")
        for resume in stored_resumes:
            col1, col2 = st.sidebar.columns([4, 1])
            with col1:
                st.write(f"ðŸ“„ {os.path.basename(resume)}")
            with col2:
                if st.button("ðŸ—‘ï¸", key=f"del_{resume}"):
                    if st.session_state.resume_analyzer.delete_resume(resume):
                        st.success(f"Deleted {resume}")
                        time.sleep(1)
                        st.rerun()
    else:
        st.sidebar.info("No resumes stored")

def main():
    st.set_page_config(
        page_title="Resume Analysis System",
        page_icon="ðŸ“„",
        layout="wide"
    )
    
    initialize_session_state()
    
    st.title("ðŸ“„ Resume Analysis System")
    
    # System Status Display
    with st.expander("System Status", expanded=True):
        col1, col2 = st.columns(2)
        with col1:
            st.write("API Status:")
            st.write(f"OpenRouter Key: {'âœ…' if settings.OPENROUTER_API_KEY else 'âŒ'}")
            st.write(f"Pinecone Key: {'âœ…' if settings.PINECONE_API_KEY else 'âŒ'}")
        with col2:
            st.write("System Status:")
            st.write(f"System Initialized: {'âœ…' if st.session_state.resume_analyzer else 'âŒ'}")
            if st.session_state.processing_status:
                success, message = st.session_state.processing_status
                st.write(f"Last Process Status: {'âœ…' if success else 'âŒ'} {message}")
    
    # Sidebar
    with st.sidebar:
        st.header("System Controls")
        
        # Initialize System Button
        if st.button("Initialize System", use_container_width=True):
            with st.spinner("Initializing..."):
                try:
                    st.session_state.resume_analyzer = ResumeAnalyzer(
                        openrouter_api_key=settings.OPENROUTER_API_KEY,
                        pinecone_api_key=settings.PINECONE_API_KEY
                    )
                    st.success("âœ… System initialized!")
                    st.session_state.processing_status = None
                    st.rerun()
                except Exception as e:
                    st.error(f"âŒ Error: {str(e)}")
        
        # File Upload Section
        st.header("Resume Upload")
        uploaded_files = st.file_uploader(
            "Upload Resumes (PDF, TXT)", 
            accept_multiple_files=True,
            type=['pdf', 'txt']
        )
        
        if uploaded_files:
            if handle_file_upload(uploaded_files):
                if st.button("Process Uploaded Resumes", use_container_width=True):
                    with st.spinner("Processing resumes..."):
                        try:
                            success, message = st.session_state.resume_analyzer.process_resumes()
                            if success:
                                st.success(f"âœ… {message}")
                                time.sleep(1)
                                st.rerun()
                            else:
                                st.error(f"âŒ {message}")
                        except Exception as e:
                            st.error(f"âŒ Error processing resumes: {str(e)}")
        
        # Display stored resumes in sidebar
        display_stored_resumes()
    
    # Main Content Area
    if st.session_state.resume_analyzer:
        st.header("Job Requirements")
        job_requirements = st.text_area(
            "Enter the job requirements:",
            value=st.session_state.job_requirements,
            height=200
        )
        
        col1, col2 = st.columns([1, 4])
        with col1:
            analyze_button = st.button("Analyze Resumes", type="primary")
        
        if analyze_button and job_requirements:
            st.session_state.job_requirements = job_requirements
            with st.spinner("Analyzing resumes..."):
                try:
                    results = asyncio.run(
                        st.session_state.resume_analyzer.analyze_resumes(job_requirements)
                    )
                    if results:
                        st.success("Analysis completed!")
                        display_analysis_results(results)
                    else:
                        st.warning("No results to display")
                except Exception as e:
                    st.error(f"âŒ Error analyzing resumes: {str(e)}")
                    logger.error(f"Analysis error: {str(e)}")
        elif analyze_button:
            st.warning("Please enter job requirements first")
    else:
        st.info("ðŸ‘ˆ Please initialize the system using the sidebar controls")

def display_analysis_results(results):
    if not results or not results.get('analysis'):
        st.warning("No analysis results to display")
        return
    
    st.write("### Analysis Results")
    
    # Display summary stats
    st.write(f"Total Resumes Analyzed: {results['total_resumes']}")
    
    # Display individual results
    for result in results['analysis']:
        with st.expander(f"ðŸ“„ {os.path.basename(result['resume_file'])} (Match Score: {result['match_score']}%)"):
            st.write("**Key Matching Qualifications:**")
            for qual in result['qualifications_match']:
                st.write(f"- {qual}")
                
            st.write("\n**Missing Requirements:**")
            for req in result['missing_requirements']:
                st.write(f"- {req}")
                
            st.write("\n**Additional Relevant Skills:**")
            for skill in result['additional_skills']:
                st.write(f"- {skill}")
                
            st.write(f"\n**Years of Experience:** {result['years_experience']}")
            st.write(f"\n**Summary:** {result['summary']}")

if __name__ == "__main__":
    main()

# File: Merge.py

def should_include_file(file_path: str, excluded_dirs: Set[str], excluded_extensions: Set[str], excluded_files: Set[str]) -> bool:
    """
    Check if a file should be included in the merge.
    """
    path = Path(file_path)
    
    # Check if file is in excluded_files
    if path.name.lower() in excluded_files:
        return False
        
    # Check if any parent directory is in excluded_dirs
    if any(part in excluded_dirs for part in path.parts):
        return False
        
    # Check file extension
    if path.suffix in excluded_extensions:
        return False
        
    # Check if it's a hidden file
    if path.name.startswith('.'):
        return False
        
    return True

def get_all_files(excluded_dirs: Set[str], excluded_extensions: Set[str], excluded_files: Set[str]) -> List[str]:
    """
    Get all files in current directory that should be included.
    """
    all_files = []
    
    for root, _, files in os.walk('.'):
        for file in files:
            file_path = os.path.join(root, file)
            if should_include_file(file_path, excluded_dirs, excluded_extensions, excluded_files):
                all_files.append(file_path)
                
    return sorted(all_files)  # Sort for consistent output

def create_merged_file() -> None:
    """
    Merge all files from current directory into merged_project.txt
    """
    output_file = 'merged_project.txt'
    
    # Define exclusions
    excluded_dirs = {'__pycache__', 'venv', '.git', '.idea', 'node_modules', '.pytest_cache'}
    excluded_extensions = {'.pyc', '.pyo', '.pyd', '.so', '.dll', '.dylib'}
    excluded_files = {'readme.md', 'merge.py', output_file.lower(), '.gitignore', 'requirements.txt', '.env', '.env.template'}
    
    files = get_all_files(excluded_dirs, excluded_extensions, excluded_files)
    
    with open(output_file, 'w', encoding='utf-8') as outfile:
        # Write header
        outfile.write(f"# Merged File Contents\n")
        outfile.write(f"# Generated on: {datetime.datetime.now()}\n")
        outfile.write(f"# Source Directory: {os.path.abspath('.')}\n\n")
        
        for file_path in files:
            try:
                # Write file header
                outfile.write(f"\n{'='*80}\n")
                outfile.write(f"# File: {file_path}\n")
                outfile.write(f"{'='*80}\n\n")
                
                # Write file contents
                with open(file_path, 'r', encoding='utf-8') as infile:
                    content = infile.read()
                    outfile.write(content)
                    
                # Add newline after each file
                outfile.write('\n')
                
            except Exception as e:
                outfile.write(f"# Error reading file {file_path}: {str(e)}\n")

def main():
    create_merged_file()
    print("Files merged successfully into merged_project.txt")

if __name__ == "__main__":
    main()

# File: resume_analyzer_complete.py
"""
Resume Analyzer - Complete Project
Generated on: 2025-02-24 09:16:58
This file contains all merged Python code from the Resume Analyzer project.
"""

# ====================== Import Statements ======================



# ====================== CONFIG ======================

# File: config\settings.py
# resume_analyzer/config/settings.py

class Settings:
    def __init__(self):
        # Load environment variables
        load_dotenv()
        
        # API Keys
        self.OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY')
        self.PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')
        
        # DeepSeek Model Settings
        self.DEEPSEEK_MODEL = os.getenv('DEEPSEEK_MODEL', 'deepseek/deepseek-r1:nitro')
        self.DEEPSEEK_TEMPERATURE = float(os.getenv('DEEPSEEK_TEMPERATURE', '0.5'))
        self.DEEPSEEK_API_BASE = os.getenv('DEEPSEEK_API_BASE', 'https://openrouter.ai/api/v1')
        
        # Pinecone Settings
        self.PINECONE_INDEX_NAME = os.getenv('PINECONE_INDEX_NAME', 'resume-analysis')
        self.PINECONE_DIMENSION = int(os.getenv('PINECONE_DIMENSION', '768'))
        self.PINECONE_METRIC = os.getenv('PINECONE_METRIC', 'cosine')
        self.PINECONE_CLOUD = os.getenv('PINECONE_CLOUD', 'aws')
        self.PINECONE_REGION = os.getenv('PINECONE_REGION', 'us-east-1')
        
        # Embeddings Settings
        self.EMBEDDINGS_MODEL = os.getenv('EMBEDDINGS_MODEL', 'sentence-transformers/all-mpnet-base-v2')
        self.EMBEDDINGS_DEVICE = os.getenv('EMBEDDINGS_DEVICE', 'cpu').strip()  # Added .strip() to remove any whitespace
        
        # Document Processing Settings
        self.CHUNK_SIZE = int(os.getenv('CHUNK_SIZE', '1000'))
        self.CHUNK_OVERLAP = int(os.getenv('CHUNK_OVERLAP', '100'))
        
        # Data Directory
        self.RESUME_DIR = os.getenv('RESUME_DIR', 'resumes')

# Create a single instance to be imported by other modules
settings = Settings()

# Make settings available for import
__all__ = ['settings']


# File: config\__init__.py
# resume_analyzer/config/__init__.py
"""Configuration package for the Resume Analysis System."""



# ====================== UTILS ======================

# File: utils\logging_config.py

def setup_logging():
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG)
    
    # Console handler
    ch = logging.StreamHandler(sys.stdout)
    ch.setLevel(logging.DEBUG)
    
    # File handler
    fh = logging.FileHandler('app_debug.log')
    fh.setLevel(logging.DEBUG)

    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    ch.setFormatter(formatter)
    fh.setFormatter(formatter)

    logger.addHandler(ch)
    logger.addHandler(fh)
    return logger

logger = setup_logging()


# File: utils\__init__.py
# resume_analyzer/utils/__init__.py
"""Utilities package for the Resume Analysis System."""



# ====================== MODELS ======================

# File: models\deepseek_llm.py
# models/deepseek_llm.py

class OpenRouterDeepSeek:
    """Class for interacting with OpenRouter's DeepSeek model"""
    
    def __init__(self, api_key: str):
        """Initialize with OpenRouter API key"""
        self.api_key = api_key
        self.api_base = settings.DEEPSEEK_API_BASE
        self.model = settings.DEEPSEEK_MODEL
        self.temperature = settings.DEEPSEEK_TEMPERATURE

    async def ainvoke(self, prompt: str) -> str:
        """
        Asynchronously invoke the DeepSeek model with a prompt
        
        Args:
            prompt (str): The prompt to send to the model
            
        Returns:
            str: The model's response text
            
        Raises:
            Exception: If the API call fails
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://resume-analyzer.example.com",  # Replace with your domain
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.temperature
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.api_base,
                    headers=headers,
                    json=payload
                ) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        raise Exception(f"API request failed: {error_text}")
                    
                    result = await response.json()
                    return result["choices"][0]["message"]["content"]
                    
        except Exception as e:
            logger.error(f"Error calling OpenRouter API: {str(e)}")
            raise

# File: models\setup.py

setup(
    name="resume_analyzer",
    version="1.0.0",
    packages=find_packages(),
    install_requires=[
        "streamlit",
        "python-dotenv",
        "langchain",
        "langchain-community",
        "langchain-core",
        "pinecone",  # Updated from pinecone-client
        "sentence-transformers",
        "transformers",
        "torch",
        "pypdf",
        "huggingface-hub",
        "nltk",
        "unstructured",  # For document loading
        "markdown",      # For markdown processing
        "aiohttp",      # For async HTTP requests
    ],
)

# File: models\__init__.py
# models/__init__.py
"""Models package for the Resume Analysis System."""


__all__ = ['OpenRouterDeepSeek']


# ====================== SERVICES ======================

# File: services\document_processor.py

class EnhancedDocumentProcessor:
    """Enhanced document processing with smart chunking strategies"""
    
    def __init__(self):
        # Download required NLTK data
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            nltk.download('punkt')
    
    def create_chunks(self, text: str, metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Create smart chunks based on document content"""
        # First try header-based splitting for structured documents
        headers_to_split_on = [
            ("#", "Header 1"),
            ("##", "Header 2"),
            ("###", "Header 3"),
        ]
        
        markdown_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=headers_to_split_on
        )
        
        try:
            md_chunks = markdown_splitter.split_text(text)
            if len(md_chunks) > 1:  # If we found headers
                return self._process_markdown_chunks(md_chunks, metadata)
        except Exception as e:
            logger.warning(f"Markdown splitting failed: {str(e)}")
        
        # Fall back to sentence-based splitting
        return self._create_semantic_chunks(text, metadata)
    
    def _process_markdown_chunks(
        self, 
        md_chunks: List[Any], 
        metadata: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Process markdown chunks with headers"""
        processed_chunks = []
        
        for chunk in md_chunks:
            chunk_metadata = metadata.copy()
            # Add header information to metadata
            for header_level, header_text in chunk.metadata.items():
                chunk_metadata[f"header_{header_level}"] = header_text
            
            processed_chunks.append({
                "text": chunk.page_content,
                "metadata": chunk_metadata
            })
        
        return processed_chunks
    
    def _create_semantic_chunks(
        self, 
        text: str, 
        metadata: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Create chunks based on semantic boundaries"""
        # First split into sentences
        sentences = sent_tokenize(text)
        
        # Group sentences into chunks
        chunks = []
        current_chunk = []
        current_length = 0
        
        for sentence in sentences:
            sentence_length = len(sentence)
            
            if current_length + sentence_length > settings.CHUNK_SIZE:
                if current_chunk:  # Save current chunk if it exists
                    chunks.append({
                        "text": " ".join(current_chunk),
                        "metadata": metadata.copy()
                    })
                current_chunk = [sentence]
                current_length = sentence_length
            else:
                current_chunk.append(sentence)
                current_length += sentence_length
        
        # Add the last chunk if it exists
        if current_chunk:
            chunks.append({
                "text": " ".join(current_chunk),
                "metadata": metadata.copy()
            })
        
        return chunks
    
    def extract_metadata(self, text: str, filename: str) -> Dict[str, Any]:
        """Extract relevant metadata from document content"""
        metadata = {
            "source": filename,
            "char_count": len(text),
            "estimated_read_time": len(text.split()) / 200  # Assuming 200 wpm reading speed
        }
        
        # Try to identify document sections
        sections = []
        current_section = ""
        for line in text.split('\n'):
            if line.strip().startswith('#'):
                if current_section:
                    sections.append(current_section)
                current_section = line.strip('#').strip()
        if current_section:
            sections.append(current_section)
        
        if sections:
            metadata["sections"] = sections
        
        return metadata

# File: services\resume_analyzer.py
# resume_analyzer/services/resume_analyzer.py


class ResumeAnalyzer:
    def __init__(self, openrouter_api_key: str, pinecone_api_key: str):
        """Initialize the Resume Analyzer with required API keys"""
        logger.info("Initializing ResumeAnalyzer")
        self.openrouter_api_key = openrouter_api_key
        self.vector_store = VectorStoreService(pinecone_api_key)
        self.llm = OpenRouterDeepSeek(api_key=openrouter_api_key)
        
        # Initialize vector store and process existing resumes
        try:
            self.vectorstore = self._initialize_vectorstore()
        except Exception as e:
            logger.error(f"Error initializing vector store: {str(e)}")
            raise

    def _initialize_vectorstore(self):
        """Initialize vector store and process existing resumes"""
        # Initialize Pinecone connection
        index = self.vector_store.initialize_store()
        
        # Create VectorStore instance
        if index:
            return PineconeVectorStore(
                index=index,
                embedding=self.vector_store.embeddings,
                text_key="text"
            )
        return None

    def process_new_resumes(self, data_dir: str = settings.RESUME_DIR) -> Tuple[bool, str]:
        """Process only newly added resumes"""
        try:
            logger.info(f"Processing new resumes from directory: {data_dir}")
            
            if not os.path.exists(data_dir):
                logger.error(f"Directory not found: {data_dir}")
                return False, "Resume directory not found"
            
            # Get list of files and currently stored resumes
            files = [f for f in os.listdir(data_dir) 
                    if os.path.isfile(os.path.join(data_dir, f))]
            stored_resumes = self.vector_store.list_documents()
            
            # Filter for new files
            new_files = [f for f in files 
                        if os.path.join(data_dir, f) not in stored_resumes]
            
            if not new_files:
                logger.info("No new resumes to process")
                return True, "No new resumes to process"
            
            logger.info(f"Found {len(new_files)} new files to process")
            
            # Process new files
            documents = []
            for file in new_files:
                file_path = os.path.join(data_dir, file)
                try:
                    if file.lower().endswith('.pdf'):
                        loader = PyPDFLoader(file_path)
                    else:
                        loader = TextLoader(file_path)
                    documents.extend(loader.load())
                except Exception as e:
                    logger.error(f"Error loading {file}: {str(e)}")
            
            if not documents:
                return False, "No new resumes could be processed"
            
            # Create chunks and update vector store
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=settings.CHUNK_SIZE,
                chunk_overlap=settings.CHUNK_OVERLAP,
                length_function=len,
                add_start_index=True,
            )
            chunks = text_splitter.split_documents(documents)
            
            # Update vector store
            self.vectorstore = self.vector_store.create_from_documents(chunks)
            
            return True, f"Successfully processed {len(new_files)} new resumes"
            
        except Exception as e:
            logger.error(f"Error processing new resumes: {str(e)}")
            return False, f"Error processing new resumes: {str(e)}"

    def list_stored_resumes(self) -> List[str]:
        """Return list of resumes stored in the vector database"""
        return self.vector_store.list_documents()
    
    def delete_resume(self, resume_filename: str) -> bool:
        """Delete a specific resume from the vector database"""
        success = self.vector_store.delete_document(resume_filename)
        if success:
            # Reinitialize vector store after deletion
            try:
                self.vectorstore = self._initialize_vectorstore()
            except Exception as e:
                logger.error(f"Error reinitializing vector store after deletion: {str(e)}")
        return success
    async def analyze_resumes(self, job_requirements: str) -> Dict[str, Any]:
        """
        Analyze resumes against job requirements
        
        Args:
            job_requirements: String containing job requirements
            
        Returns:
            Dictionary containing analysis results and metadata
            
        Raises:
            ValueError: If vector store is not initialized
            Exception: If analysis fails
        """
        try:
            if not self.vectorstore:
                logger.error("Vector store not initialized")
                raise ValueError("Vector store not initialized. Please process resumes first.")

            logger.info("Starting resume analysis...")
            # Retrieve relevant resume chunks
            docs = self.vectorstore.similarity_search(job_requirements, k=10)
            logger.info(f"Retrieved {len(docs)} relevant chunks")
            
            # Group chunks by resume
            resume_contents = {}
            for doc in docs:
                source = doc.metadata.get('source', 'Unknown')
                if source not in resume_contents:
                    resume_contents[source] = []
                resume_contents[source].append(doc.page_content)
            
            logger.info(f"Analyzing {len(resume_contents)} resumes")
            
            # Analyze each resume
            analysis_results = []
            for resume_file, contents in resume_contents.items():
                logger.info(f"Analyzing resume: {resume_file}")
                full_content = "\n".join(contents)
                
                analysis_prompt = f"""
                Act as an expert HR analyst. Analyze this resume against the following job requirements. 
                Provide a structured analysis in valid JSON format.
                
                Job Requirements:
                {job_requirements}
                
                Resume Content:
                {full_content}
                
                Response Instructions:
                1. Match Score: Provide a number between 0-100
                2. List 3-5 key matching qualifications
                3. List 2-3 missing requirements
                4. List 2-3 additional relevant skills
                5. Calculate years of relevant experience
                6. Write a 2-3 sentence summary
                
                Required JSON Structure:
                {{
                    "match_score": <number>,
                    "qualifications_match": ["qual1", "qual2", "qual3"],
                    "missing_requirements": ["req1", "req2"],
                    "additional_skills": ["skill1", "skill2"],
                    "years_experience": <number>,
                    "summary": "Brief summary text"
                }}
                
                The response must be ONLY valid JSON with no additional text.
                """
                
                try:
                    result = await self.llm.ainvoke(analysis_prompt)
                    logger.debug(f"Raw LLM response: {result}")
                    
                    # Clean and parse the response
                    result_text = result.strip()
                    if not result_text.startswith('{'):
                        result_text = result_text[result_text.find('{'):]
                    if not result_text.endswith('}'):
                        result_text = result_text[:result_text.rfind('}')+1]
                    
                    parsed_result = json.loads(result_text)
                    
                    # Validate required fields
                    required_fields = [
                        'match_score', 
                        'qualifications_match', 
                        'missing_requirements',
                        'additional_skills', 
                        'years_experience', 
                        'summary'
                    ]
                    
                    for field in required_fields:
                        if field not in parsed_result:
                            raise ValueError(f"Missing required field: {field}")
                    
                    parsed_result["resume_file"] = resume_file
                    analysis_results.append(parsed_result)
                    logger.info(f"Successfully analyzed resume: {resume_file}")
                    
                except Exception as e:
                    logger.error(f"Error analyzing {resume_file}: {str(e)}")
                    continue
            
            if not analysis_results:
                raise Exception("No resumes could be analyzed successfully")
            
            # Sort results by match score
            sorted_results = sorted(
                analysis_results, 
                key=lambda x: x.get('match_score', 0), 
                reverse=True
            )
            
            logger.info("Analysis completed successfully")
            return {
                "analysis": sorted_results,
                "total_resumes": len(resume_contents)
            }
            
        except Exception as e:
            logger.error(f"Error analyzing resumes: {str(e)}")
            raise

# File: services\vector_store.py
# services/vector_store.py


class VectorStoreService:
    """Service for managing vector store operations using Pinecone"""
    
    def __init__(self, api_key: str):
        """
        Initialize the VectorStoreService.
        
        Args:
            api_key (str): Pinecone API key
        """
        self.api_key = api_key
        self.pc = Pinecone(api_key=api_key)
        self.index_name = settings.PINECONE_INDEX_NAME
        self.embeddings = self._setup_embeddings()
        self.document_processor = EnhancedDocumentProcessor()
        self._index = None

    def _setup_embeddings(self) -> HuggingFaceEmbeddings:
        """
        Set up the HuggingFace embeddings model.
        
        Returns:
            HuggingFaceEmbeddings: Configured embeddings model
        """
        try:
            return HuggingFaceEmbeddings(
                model_name=settings.EMBEDDINGS_MODEL,
                model_kwargs={'device': settings.EMBEDDINGS_DEVICE}
            )
        except Exception as e:
            logger.error(f"Error setting up embeddings: {str(e)}")
            raise

    def initialize_store(self) -> Optional[Any]:
        """
        Initialize or connect to Pinecone index.
        
        Returns:
            Optional[Any]: Initialized Pinecone index or None if initialization fails
        """
        try:
            existing_indexes = self.pc.list_indexes()
            
            if self.index_name not in existing_indexes.names():
                logger.info(f"Creating new index: {self.index_name}")
                self.pc.create_index(
                    name=self.index_name,
                    dimension=settings.PINECONE_DIMENSION,
                    metric=settings.PINECONE_METRIC,
                    spec=ServerlessSpec(
                        cloud=settings.PINECONE_CLOUD,
                        region=settings.PINECONE_REGION
                    )
                )
                # Wait for index to be ready
                while self.index_name not in self.pc.list_indexes().names():
                    time.sleep(1)
            else:
                logger.info(f"Using existing index: {self.index_name}")
            
            self._index = self.pc.Index(self.index_name)
            if self._index:
                logger.info("Successfully connected to Pinecone index")
                return self._index
            return None
            
        except Exception as e:
            logger.error(f"Pinecone initialization failed: {str(e)}")
            raise

    def list_documents(self) -> List[str]:
        """
        List all unique document sources stored in the vector database.
        
        Returns:
            List[str]: List of document source paths
        """
        try:
            if not self._index:
                self._index = self.pc.Index(self.index_name)
            
            # Query for all vectors and fetch metadata
            query_response = self._index.query(
                vector=[0] * settings.PINECONE_DIMENSION,  # Dummy vector for metadata query
                top_k=10000,  # Adjust based on your needs
                include_metadata=True
            )
            
            # Extract unique source files from metadata
            unique_sources = set()
            for match in query_response['matches']:
                if 'metadata' in match and 'source' in match['metadata']:
                    unique_sources.add(match['metadata']['source'])
            
            return sorted(list(unique_sources))
            
        except Exception as e:
            logger.error(f"Error listing documents: {str(e)}")
            return []

    def delete_document(self, document_path: str) -> bool:
        """
        Delete all vectors associated with a specific document.
        
        Args:
            document_path (str): Path of the document to delete
            
        Returns:
            bool: True if deletion was successful, False otherwise
        """
        try:
            if not self._index:
                self._index = self.pc.Index(self.index_name)

            logger.info(f"Attempting to delete document: {document_path}")
            
            # Find all vectors associated with this document
            query_response = self._index.query(
                vector=[0] * settings.PINECONE_DIMENSION,
                top_k=10000,
                include_metadata=True
            )
            
            # Get IDs of vectors to delete
            ids_to_delete = [
                match['id'] for match in query_response['matches']
                if match.get('metadata', {}).get('source') == document_path
            ]
            
            if ids_to_delete:
                # Delete vectors in batches
                batch_size = 100
                for i in range(0, len(ids_to_delete), batch_size):
                    batch = ids_to_delete[i:i + batch_size]
                    self._index.delete(ids=batch)
                
                logger.info(f"Successfully deleted {len(ids_to_delete)} vectors for {document_path}")
                return True
            else:
                logger.warning(f"No vectors found for document: {document_path}")
                return False
            
        except Exception as e:
            logger.error(f"Error deleting document {document_path}: {str(e)}")
            return False

    def create_from_documents(self, documents: List[Any]) -> Optional[PineconeVectorStore]:
        """
        Create or update vector store from documents with enhanced processing.
        
        Args:
            documents: List of document objects to process
            
        Returns:
            Optional[PineconeVectorStore]: Initialized vector store or None if creation fails
        """
        try:
            logger.info("Starting document processing...")
            processed_chunks = []
            
            for doc in documents:
                try:
                    source = doc.metadata.get('source', 'Unknown')
                    logger.info(f"Processing document: {source}")
                    
                    # Create chunks with metadata
                    chunk = {
                        "text": doc.page_content,
                        "metadata": {
                            "source": source,
                            "start_index": doc.metadata.get('start_index', 0),
                            "chunk_type": "document",
                            "processing_date": time.strftime("%Y-%m-%d %H:%M:%S")
                        }
                    }
                    processed_chunks.append(chunk)
                    
                except Exception as e:
                    logger.error(f"Error processing document {source}: {str(e)}")
                    continue
            
            if not processed_chunks:
                raise ValueError("No documents were successfully processed")
            
            logger.info(f"Successfully processed {len(processed_chunks)} chunks")
            
            # Create embeddings
            logger.info("Creating embeddings...")
            texts = [chunk["text"] for chunk in processed_chunks]
            metadatas = [chunk["metadata"] for chunk in processed_chunks]
            
            embeddings = self.embeddings.embed_documents(texts)
            logger.info(f"Created {len(embeddings)} embeddings")
            
            # Initialize Pinecone
            index = self.initialize_store()
            if not index:
                raise ValueError("Failed to initialize Pinecone index")
            
            # Batch upload to Pinecone
            logger.info("Uploading to Pinecone...")
            batch_size = 100
            total_uploaded = 0
            
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                batch_embeddings = embeddings[i:i + batch_size]
                batch_metadata = metadatas[i:i + batch_size]
                
                vectors = [
                    (f"vec_{total_uploaded + j}", emb, {"text": text, **meta})
                    for j, (emb, text, meta) in enumerate(zip(batch_embeddings, batch_texts, batch_metadata))
                ]
                
                index.upsert(vectors=vectors)
                total_uploaded += len(batch_texts)
                logger.info(f"Uploaded batch: {total_uploaded}/{len(texts)} vectors")
            
            logger.info("Successfully created vector store")
            return PineconeVectorStore(
                index=index,
                embedding=self.embeddings,
                text_key="text"
            )
            
        except Exception as e:
            logger.error(f"Error creating vector store: {str(e)}")
            raise

    def similarity_search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """
        Perform similarity search in the vector store.
        
        Args:
            query (str): Query text to search for
            k (int): Number of results to return
            
        Returns:
            List[Dict[str, Any]]: List of similar documents with metadata
        """
        try:
            if not self._index:
                raise ValueError("Vector store not initialized")
            
            # Generate query embedding
            query_embedding = self.embeddings.embed_query(query)
            
            # Perform similarity search
            results = self._index.query(
                vector=query_embedding,
                top_k=k,
                include_metadata=True
            )
            
            # Format results
            formatted_results = []
            for match in results['matches']:
                formatted_results.append({
                    'text': match['metadata'].get('text', ''),
                    'score': match['score'],
                    'metadata': {
                        k: v for k, v in match['metadata'].items()
                        if k != 'text'
                    }
                })
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"Error performing similarity search: {str(e)}")
            raise



# File: run.py

# Add the project root to Python path
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Import and run the Streamlit app
if hasattr(app, 'main'):
    app.main()

# File: setup.py

setup(
    name="resume_analyzer",
    version="1.0.0",
    packages=find_packages(),
    install_requires=[
        "streamlit",
        "python-dotenv",
        "langchain",
        "langchain-community",
        "langchain-core",
        "langchain-pinecone",  # Added this package
        "pinecone>=0.8.0",
        "sentence-transformers",
        "transformers",
        "torch",
        "pypdf",
        "huggingface-hub",
        "nltk",
        "unstructured",
        "markdown",
        "aiohttp",
    ],
)

# File: test.py
st.write("Hello World")

# File: verify.py

print("All critical packages imported successfully!")

# File: __init__.py
# resume_analyzer/services/__init__.py
"""Services package for the Resume Analysis System."""


__all__ = ['EnhancedDocumentProcessor', 'VectorStoreService', 'ResumeAnalyzer']



================================================================================
# File: .\resumes\Alex.txt
================================================================================

ALEX RODRIGUEZ
Junior Software Developer
alex.rodriguez@email.com | (555) 123-4567
github.com/alexrod | linkedin.com/in/alexrod

SUMMARY
Recent computer science graduate with hands-on experience in web development and mobile applications. Passionate about creating user-friendly applications and learning new technologies.

SKILLS
- Languages: Python, JavaScript, Java
- Frontend: React.js, HTML5, CSS3
- Backend: Node.js, Express.js
- Databases: MongoDB, MySQL
- Tools: Git, VS Code, Docker
- Testing: Jest, PyTest

PROJECTS
E-commerce Platform (Personal Project)
- Developed a full-stack e-commerce website using MERN stack (MongoDB, Express.js, React.js, Node.js)
- Implemented user authentication, shopping cart functionality, and payment integration
- Deployed application using Docker and AWS
- GitHub: github.com/alexrod/shopwise

Weather Dashboard (Bootcamp Project)
- Built a weather dashboard using React.js and OpenWeatherMap API
- Implemented geolocation features and 5-day weather forecasts
- Utilized localStorage for saving user preferences
- GitHub: github.com/alexrod/weatherapp

EDUCATION
Bachelor of Science in Computer Science
University of Technology
Graduated: May 2023
GPA: 3.5/4.0

CERTIFICATIONS
- AWS Certified Cloud Practitioner
- MongoDB Basics Certification

LANGUAGES
English (Native), Spanish (Conversational)

================================================================================
# File: .\resumes\David.txt
================================================================================

DAVID KIM
Junior Backend Developer
david.kim@email.com | (555) 456-7890
github.com/davidkim | linkedin.com/in/davidkim

SUMMARY
Detail-oriented backend developer with strong focus on API development and database optimization. Passionate about creating efficient and scalable server-side solutions.

SKILLS
- Languages: Python, Java, SQL
- Backend: Flask, FastAPI, Spring Boot
- Databases: PostgreSQL, Redis, MySQL
- Message Queues: RabbitMQ, Redis
- Tools: Git, Docker, Postman
- Testing: PyTest, JUnit
- Monitoring: Grafana, Prometheus

PROJECTS
API Gateway Service
- Developed a microservices API gateway using FastAPI and Redis
- Implemented rate limiting and request caching
- Added comprehensive API documentation using Swagger
- GitHub: github.com/davidkim/api-gateway

Data Processing Pipeline
- Built a data processing pipeline using Python and RabbitMQ
- Implemented parallel processing for large datasets
- Created monitoring dashboard using Grafana
- GitHub: github.com/davidkim/data-pipeline

Inventory Management System
- Developed an inventory management system using Spring Boot
- Implemented real-time stock tracking and alerts
- Added barcode scanning functionality
- GitHub: github.com/davidkim/inventory-sys

EDUCATION
Bachelor of Science in Computer Engineering
Tech State University
Graduated: August 2023
GPA: 3.4/4.0

CERTIFICATIONS
- Python Professional Certification
- PostgreSQL Administration Basics

LANGUAGES
English (Native), Korean (Fluent)

================================================================================
# File: .\resumes\Shashank.txt
================================================================================

Shashank G
Junior Frontend Developer
emily.brown@email.com | 998612344
github.com/emilybrown | linkedin.com/in/emilybrown

SUMMARY
Creative frontend developer with strong design background. Focused on creating intuitive and accessible user interfaces with modern web technologies.

SKILLS
- Languages: JavaScript, HTML5, CSS3, TypeScript
- Frontend: React.js, Angular, Sass
- UI Frameworks: Material-UI, Tailwind CSS
- Version Control: Git, GitHub
- Design Tools: Figma, Adobe XD
- Testing: Jest, React Testing Library
- Performance: Lighthouse, WebPageTest

PROJECTS
Portfolio Website Generator
- Created a customizable portfolio website generator using React and Tailwind CSS
- Implemented drag-and-drop interface for easy customization
- Added responsive design templates
- GitHub: github.com/emilybrown/portfolio-gen

Accessibility Dashboard
- Developed an accessibility testing dashboard using Angular
- Implemented automated WCAG 2.1 compliance checking
- Created detailed reporting system
- GitHub: github.com/emilybrown/a11y-dash

Interactive Learning Platform
- Built an interactive learning platform using React and Material-UI
- Implemented progress tracking and achievement system
- Added interactive coding challenges
- GitHub: github.com/emilybrown/learn-interactive

EDUCATION
Bachelor of Arts in Interactive Design
Creative Arts University
Graduated: April 2023
GPA: 3.8/4.0

CERTIFICATIONS
- Web Accessibility Fundamentals (W3C)
- React Development Certification

LANGUAGES
English (Native), French (Basic)

================================================================================
# File: .\run.py
================================================================================

import os
import sys

# Add the project root to Python path
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# Import and run the Streamlit app
import app
if hasattr(app, 'main'):
    app.main()

================================================================================
# File: .\services\__init__.py
================================================================================

# resume_analyzer/services/__init__.py
"""Services package for the Resume Analysis System."""

from .resume_analyzer import ResumeAnalyzer
from .vector_store import VectorStoreService
from .document_processor import EnhancedDocumentProcessor

__all__ = ['ResumeAnalyzer', 'VectorStoreService', 'EnhancedDocumentProcessor']

================================================================================
# File: .\services\document_processor.py
================================================================================

from typing import List, Dict, Any
from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter
from langchain_community.document_loaders import UnstructuredFileLoader
import nltk
from nltk.tokenize import sent_tokenize
from utils.logging_config import logger
from config.settings import settings

class EnhancedDocumentProcessor:
    """Enhanced document processing with smart chunking strategies"""
    
    def __init__(self):
        # Download required NLTK data
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            nltk.download('punkt')
    
    def create_chunks(self, text: str, metadata: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Create smart chunks based on document content"""
        # First try header-based splitting for structured documents
        headers_to_split_on = [
            ("#", "Header 1"),
            ("##", "Header 2"),
            ("###", "Header 3"),
        ]
        
        markdown_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=headers_to_split_on
        )
        
        try:
            md_chunks = markdown_splitter.split_text(text)
            if len(md_chunks) > 1:  # If we found headers
                return self._process_markdown_chunks(md_chunks, metadata)
        except Exception as e:
            logger.warning(f"Markdown splitting failed: {str(e)}")
        
        # Fall back to sentence-based splitting
        return self._create_semantic_chunks(text, metadata)
    
    def _process_markdown_chunks(
        self, 
        md_chunks: List[Any], 
        metadata: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Process markdown chunks with headers"""
        processed_chunks = []
        
        for chunk in md_chunks:
            chunk_metadata = metadata.copy()
            # Add header information to metadata
            for header_level, header_text in chunk.metadata.items():
                chunk_metadata[f"header_{header_level}"] = header_text
            
            processed_chunks.append({
                "text": chunk.page_content,
                "metadata": chunk_metadata
            })
        
        return processed_chunks
    
    def _create_semantic_chunks(
        self, 
        text: str, 
        metadata: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Create chunks based on semantic boundaries"""
        # First split into sentences
        sentences = sent_tokenize(text)
        
        # Group sentences into chunks
        chunks = []
        current_chunk = []
        current_length = 0
        
        for sentence in sentences:
            sentence_length = len(sentence)
            
            if current_length + sentence_length > settings.CHUNK_SIZE:
                if current_chunk:  # Save current chunk if it exists
                    chunks.append({
                        "text": " ".join(current_chunk),
                        "metadata": metadata.copy()
                    })
                current_chunk = [sentence]
                current_length = sentence_length
            else:
                current_chunk.append(sentence)
                current_length += sentence_length
        
        # Add the last chunk if it exists
        if current_chunk:
            chunks.append({
                "text": " ".join(current_chunk),
                "metadata": metadata.copy()
            })
        
        return chunks
    
    def extract_metadata(self, text: str, filename: str) -> Dict[str, Any]:
        """Extract relevant metadata from document content"""
        metadata = {
            "source": filename,
            "char_count": len(text),
            "estimated_read_time": len(text.split()) / 200  # Assuming 200 wpm reading speed
        }
        
        # Try to identify document sections
        sections = []
        current_section = ""
        for line in text.split('\n'):
            if line.strip().startswith('#'):
                if current_section:
                    sections.append(current_section)
                current_section = line.strip('#').strip()
        if current_section:
            sections.append(current_section)
        
        if sections:
            metadata["sections"] = sections
        
        return metadata

================================================================================
# File: .\services\resume_analyzer.py
================================================================================

# resume_analyzer/services/resume_analyzer.py
import json
import os
from typing import Dict, Any, List, Tuple
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader
import streamlit as st
import time

from config.settings import settings
from utils.logging_config import logger
from models.deepseek_llm import OpenRouterDeepSeek
from services.vector_store import VectorStoreService
from langchain_pinecone import PineconeVectorStore

class ResumeAnalyzer:
    def __init__(self, openrouter_api_key: str, pinecone_api_key: str):
        """Initialize the Resume Analyzer with required API keys"""
        logger.info("Initializing ResumeAnalyzer")
        self.openrouter_api_key = openrouter_api_key
        self.vector_store = VectorStoreService(pinecone_api_key)
        self.llm = OpenRouterDeepSeek(api_key=openrouter_api_key)
        
        # Initialize vector store and process existing resumes
        try:
            self.vectorstore = self._initialize_vectorstore()
        except Exception as e:
            logger.error(f"Error initializing vector store: {str(e)}")
            raise

    def _initialize_vectorstore(self):
        """Initialize vector store and process existing resumes"""
        # Initialize Pinecone connection
        index = self.vector_store.initialize_store()
        
        # Create VectorStore instance
        if index:
            return PineconeVectorStore(
                index=index,
                embedding=self.vector_store.embeddings,
                text_key="text"
            )
        return None

    def process_new_resumes(self, data_dir: str = settings.RESUME_DIR) -> Tuple[bool, str]:
        """Process only newly added resumes"""
        try:
            logger.info(f"Processing new resumes from directory: {data_dir}")
            
            if not os.path.exists(data_dir):
                logger.error(f"Directory not found: {data_dir}")
                return False, "Resume directory not found"
            
            # Get list of files and currently stored resumes
            files = [f for f in os.listdir(data_dir) 
                    if os.path.isfile(os.path.join(data_dir, f))]
            stored_resumes = self.vector_store.list_documents()
            
            # Filter for new files
            new_files = [f for f in files 
                        if os.path.join(data_dir, f) not in stored_resumes]
            
            if not new_files:
                logger.info("No new resumes to process")
                return True, "No new resumes to process"
            
            logger.info(f"Found {len(new_files)} new files to process")
            
            # Process new files
            documents = []
            for file in new_files:
                file_path = os.path.join(data_dir, file)
                try:
                    if file.lower().endswith('.pdf'):
                        loader = PyPDFLoader(file_path)
                    else:
                        loader = TextLoader(file_path)
                    documents.extend(loader.load())
                except Exception as e:
                    logger.error(f"Error loading {file}: {str(e)}")
            
            if not documents:
                return False, "No new resumes could be processed"
            
            # Create chunks and update vector store
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=settings.CHUNK_SIZE,
                chunk_overlap=settings.CHUNK_OVERLAP,
                length_function=len,
                add_start_index=True,
            )
            chunks = text_splitter.split_documents(documents)
            
            # Update vector store
            self.vectorstore = self.vector_store.create_from_documents(chunks)
            
            return True, f"Successfully processed {len(new_files)} new resumes"
            
        except Exception as e:
            logger.error(f"Error processing new resumes: {str(e)}")
            return False, f"Error processing new resumes: {str(e)}"

    def list_stored_resumes(self) -> List[str]:
        """Return list of resumes stored in the vector database"""
        return self.vector_store.list_documents()
    
    def delete_resume(self, resume_filename: str) -> bool:
        """Delete a specific resume from the vector database"""
        success = self.vector_store.delete_document(resume_filename)
        if success:
            # Reinitialize vector store after deletion
            try:
                self.vectorstore = self._initialize_vectorstore()
            except Exception as e:
                logger.error(f"Error reinitializing vector store after deletion: {str(e)}")
        return success
    async def analyze_resumes(self, job_requirements: str) -> Dict[str, Any]:
        """
        Analyze resumes against job requirements
        
        Args:
            job_requirements: String containing job requirements
            
        Returns:
            Dictionary containing analysis results and metadata
            
        Raises:
            ValueError: If vector store is not initialized
            Exception: If analysis fails
        """
        try:
            if not self.vectorstore:
                logger.error("Vector store not initialized")
                raise ValueError("Vector store not initialized. Please process resumes first.")

            logger.info("Starting resume analysis...")
            # Retrieve relevant resume chunks
            docs = self.vectorstore.similarity_search(job_requirements, k=10)
            logger.info(f"Retrieved {len(docs)} relevant chunks")
            
            # Group chunks by resume
            resume_contents = {}
            for doc in docs:
                source = doc.metadata.get('source', 'Unknown')
                if source not in resume_contents:
                    resume_contents[source] = []
                resume_contents[source].append(doc.page_content)
            
            logger.info(f"Analyzing {len(resume_contents)} resumes")
            
            # Analyze each resume
            analysis_results = []
            for resume_file, contents in resume_contents.items():
                logger.info(f"Analyzing resume: {resume_file}")
                full_content = "\n".join(contents)
                
                analysis_prompt = f"""
                Act as an expert HR analyst. Analyze this resume against the following job requirements. 
                Provide a structured analysis in valid JSON format.
                
                Job Requirements:
                {job_requirements}
                
                Resume Content:
                {full_content}
                
                Response Instructions:
                1. Match Score: Provide a number between 0-100
                2. List 3-5 key matching qualifications
                3. List 2-3 missing requirements
                4. List 2-3 additional relevant skills
                5. Calculate years of relevant experience
                6. Write a 2-3 sentence summary
                
                Required JSON Structure:
                {{
                    "match_score": <number>,
                    "qualifications_match": ["qual1", "qual2", "qual3"],
                    "missing_requirements": ["req1", "req2"],
                    "additional_skills": ["skill1", "skill2"],
                    "years_experience": <number>,
                    "summary": "Brief summary text"
                }}
                
                The response must be ONLY valid JSON with no additional text.
                """
                
                try:
                    result = await self.llm.ainvoke(analysis_prompt)
                    logger.debug(f"Raw LLM response: {result}")
                    
                    # Clean and parse the response
                    result_text = result.strip()
                    if not result_text.startswith('{'):
                        result_text = result_text[result_text.find('{'):]
                    if not result_text.endswith('}'):
                        result_text = result_text[:result_text.rfind('}')+1]
                    
                    parsed_result = json.loads(result_text)
                    
                    # Validate required fields
                    required_fields = [
                        'match_score', 
                        'qualifications_match', 
                        'missing_requirements',
                        'additional_skills', 
                        'years_experience', 
                        'summary'
                    ]
                    
                    for field in required_fields:
                        if field not in parsed_result:
                            raise ValueError(f"Missing required field: {field}")
                    
                    parsed_result["resume_file"] = resume_file
                    analysis_results.append(parsed_result)
                    logger.info(f"Successfully analyzed resume: {resume_file}")
                    
                except Exception as e:
                    logger.error(f"Error analyzing {resume_file}: {str(e)}")
                    continue
            
            if not analysis_results:
                raise Exception("No resumes could be analyzed successfully")
            
            # Sort results by match score
            sorted_results = sorted(
                analysis_results, 
                key=lambda x: x.get('match_score', 0), 
                reverse=True
            )
            
            logger.info("Analysis completed successfully")
            return {
                "analysis": sorted_results,
                "total_resumes": len(resume_contents)
            }
            
        except Exception as e:
            logger.error(f"Error analyzing resumes: {str(e)}")
            raise

================================================================================
# File: .\services\vector_store.py
================================================================================

# services/vector_store.py
import os
import time
from typing import List, Dict, Any, Optional
from pinecone import Pinecone, ServerlessSpec
from langchain_pinecone import PineconeVectorStore
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.document_loaders import UnstructuredFileLoader

from config.settings import settings
from utils.logging_config import logger
from services.document_processor import EnhancedDocumentProcessor

class VectorStoreService:
    """Service for managing vector store operations using Pinecone"""
    
    def __init__(self, api_key: str):
        """
        Initialize the VectorStoreService.
        
        Args:
            api_key (str): Pinecone API key
        """
        self.api_key = api_key
        self.pc = Pinecone(api_key=api_key)
        self.index_name = settings.PINECONE_INDEX_NAME
        self.embeddings = self._setup_embeddings()
        self.document_processor = EnhancedDocumentProcessor()
        self._index = None

    def _setup_embeddings(self) -> HuggingFaceEmbeddings:
        """
        Set up the HuggingFace embeddings model.
        
        Returns:
            HuggingFaceEmbeddings: Configured embeddings model
        """
        try:
            return HuggingFaceEmbeddings(
                model_name=settings.EMBEDDINGS_MODEL,
                model_kwargs={'device': settings.EMBEDDINGS_DEVICE}
            )
        except Exception as e:
            logger.error(f"Error setting up embeddings: {str(e)}")
            raise

    def initialize_store(self) -> Optional[Any]:
        """
        Initialize or connect to Pinecone index.
        
        Returns:
            Optional[Any]: Initialized Pinecone index or None if initialization fails
        """
        try:
            existing_indexes = self.pc.list_indexes()
            
            if self.index_name not in existing_indexes.names():
                logger.info(f"Creating new index: {self.index_name}")
                self.pc.create_index(
                    name=self.index_name,
                    dimension=settings.PINECONE_DIMENSION,
                    metric=settings.PINECONE_METRIC,
                    spec=ServerlessSpec(
                        cloud=settings.PINECONE_CLOUD,
                        region=settings.PINECONE_REGION
                    )
                )
                # Wait for index to be ready
                while self.index_name not in self.pc.list_indexes().names():
                    time.sleep(1)
            else:
                logger.info(f"Using existing index: {self.index_name}")
            
            self._index = self.pc.Index(self.index_name)
            if self._index:
                logger.info("Successfully connected to Pinecone index")
                return self._index
            return None
            
        except Exception as e:
            logger.error(f"Pinecone initialization failed: {str(e)}")
            raise

    def list_documents(self) -> List[str]:
        """
        List all unique document sources stored in the vector database.
        
        Returns:
            List[str]: List of document source paths
        """
        try:
            if not self._index:
                self._index = self.pc.Index(self.index_name)
            
            # Query for all vectors and fetch metadata
            query_response = self._index.query(
                vector=[0] * settings.PINECONE_DIMENSION,  # Dummy vector for metadata query
                top_k=10000,  # Adjust based on your needs
                include_metadata=True
            )
            
            # Extract unique source files from metadata
            unique_sources = set()
            for match in query_response['matches']:
                if 'metadata' in match and 'source' in match['metadata']:
                    unique_sources.add(match['metadata']['source'])
            
            return sorted(list(unique_sources))
            
        except Exception as e:
            logger.error(f"Error listing documents: {str(e)}")
            return []

    def delete_document(self, document_path: str) -> bool:
        """
        Delete all vectors associated with a specific document.
        
        Args:
            document_path (str): Path of the document to delete
            
        Returns:
            bool: True if deletion was successful, False otherwise
        """
        try:
            if not self._index:
                self._index = self.pc.Index(self.index_name)

            logger.info(f"Attempting to delete document: {document_path}")
            
            # Find all vectors associated with this document
            query_response = self._index.query(
                vector=[0] * settings.PINECONE_DIMENSION,
                top_k=10000,
                include_metadata=True
            )
            
            # Get IDs of vectors to delete
            ids_to_delete = [
                match['id'] for match in query_response['matches']
                if match.get('metadata', {}).get('source') == document_path
            ]
            
            if ids_to_delete:
                # Delete vectors in batches
                batch_size = 100
                for i in range(0, len(ids_to_delete), batch_size):
                    batch = ids_to_delete[i:i + batch_size]
                    self._index.delete(ids=batch)
                
                logger.info(f"Successfully deleted {len(ids_to_delete)} vectors for {document_path}")
                return True
            else:
                logger.warning(f"No vectors found for document: {document_path}")
                return False
            
        except Exception as e:
            logger.error(f"Error deleting document {document_path}: {str(e)}")
            return False

    def create_from_documents(self, documents: List[Any]) -> Optional[PineconeVectorStore]:
        """
        Create or update vector store from documents with enhanced processing.
        
        Args:
            documents: List of document objects to process
            
        Returns:
            Optional[PineconeVectorStore]: Initialized vector store or None if creation fails
        """
        try:
            logger.info("Starting document processing...")
            processed_chunks = []
            
            for doc in documents:
                try:
                    source = doc.metadata.get('source', 'Unknown')
                    logger.info(f"Processing document: {source}")
                    
                    # Create chunks with metadata
                    chunk = {
                        "text": doc.page_content,
                        "metadata": {
                            "source": source,
                            "start_index": doc.metadata.get('start_index', 0),
                            "chunk_type": "document",
                            "processing_date": time.strftime("%Y-%m-%d %H:%M:%S")
                        }
                    }
                    processed_chunks.append(chunk)
                    
                except Exception as e:
                    logger.error(f"Error processing document {source}: {str(e)}")
                    continue
            
            if not processed_chunks:
                raise ValueError("No documents were successfully processed")
            
            logger.info(f"Successfully processed {len(processed_chunks)} chunks")
            
            # Create embeddings
            logger.info("Creating embeddings...")
            texts = [chunk["text"] for chunk in processed_chunks]
            metadatas = [chunk["metadata"] for chunk in processed_chunks]
            
            embeddings = self.embeddings.embed_documents(texts)
            logger.info(f"Created {len(embeddings)} embeddings")
            
            # Initialize Pinecone
            index = self.initialize_store()
            if not index:
                raise ValueError("Failed to initialize Pinecone index")
            
            # Batch upload to Pinecone
            logger.info("Uploading to Pinecone...")
            batch_size = 100
            total_uploaded = 0
            
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                batch_embeddings = embeddings[i:i + batch_size]
                batch_metadata = metadatas[i:i + batch_size]
                
                vectors = [
                    (f"vec_{total_uploaded + j}", emb, {"text": text, **meta})
                    for j, (emb, text, meta) in enumerate(zip(batch_embeddings, batch_texts, batch_metadata))
                ]
                
                index.upsert(vectors=vectors)
                total_uploaded += len(batch_texts)
                logger.info(f"Uploaded batch: {total_uploaded}/{len(texts)} vectors")
            
            logger.info("Successfully created vector store")
            return PineconeVectorStore(
                index=index,
                embedding=self.embeddings,
                text_key="text"
            )
            
        except Exception as e:
            logger.error(f"Error creating vector store: {str(e)}")
            raise

    def similarity_search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """
        Perform similarity search in the vector store.
        
        Args:
            query (str): Query text to search for
            k (int): Number of results to return
            
        Returns:
            List[Dict[str, Any]]: List of similar documents with metadata
        """
        try:
            if not self._index:
                raise ValueError("Vector store not initialized")
            
            # Generate query embedding
            query_embedding = self.embeddings.embed_query(query)
            
            # Perform similarity search
            results = self._index.query(
                vector=query_embedding,
                top_k=k,
                include_metadata=True
            )
            
            # Format results
            formatted_results = []
            for match in results['matches']:
                formatted_results.append({
                    'text': match['metadata'].get('text', ''),
                    'score': match['score'],
                    'metadata': {
                        k: v for k, v in match['metadata'].items()
                        if k != 'text'
                    }
                })
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"Error performing similarity search: {str(e)}")
            raise

================================================================================
# File: .\setup.py
================================================================================

from setuptools import setup, find_packages

setup(
    name="resume_analyzer",
    version="1.0.0",
    packages=find_packages(),
    install_requires=[
        "streamlit>=1.29.0",
        "python-dotenv>=1.0.0",
        "langchain>=0.1.0",
        "langchain-community>=0.0.10",
        "langchain-core>=0.1.10",
        "langchain-pinecone>=0.0.2",
        "langchain-huggingface>=0.0.2",  # Added for HuggingFace integration
        "pinecone-client>=3.0.0",  # Updated package name and version
        "sentence-transformers>=2.2.2",
        "transformers>=4.36.0",
        "--extra-index-url https://download.pytorch.org/whl/cu121",  # Added PyTorch CUDA channel
        "torch==2.2.0",  # Fixed version
        "torchvision==0.17.0",
        "torchaudio==2.2.0",
        "pypdf>=3.17.0",
        "huggingface-hub>=0.20.0",
        "nltk>=3.8.1",
        "unstructured>=0.11.0",
        "markdown>=3.5.0",
        "aiohttp>=3.9.0",
        "nest-asyncio>=1.5.8",
        "asyncio>=3.4.3",
        "packaging>=23.2",  # Added for better package compatibility
        "typing-extensions>=4.9.0",  # Added for type hints support
    ],
    python_requires=">=3.9,<3.12",  # Restricted Python version range
)

================================================================================
# File: .\startup.py
================================================================================

import os
os.environ['PYTORCH_JIT'] = '0'  # Disable JIT
import torch
torch.set_grad_enabled(False)  # Disable gradients

# Patch torch._classes to avoid path issues
import torch._classes
def mock_getattr(*args, **kwargs):
    return None
torch._classes.__getattr__ = mock_getattr

================================================================================
# File: .\test.py
================================================================================

import streamlit as st
st.write("Hello World")

================================================================================
# File: .\utils\__init__.py
================================================================================

# resume_analyzer/utils/__init__.py
"""Utilities package for the Resume Analysis System."""

from .logging_config import logger

================================================================================
# File: .\utils\async_utils.py
================================================================================

import asyncio
from functools import wraps
import streamlit as st

def ensure_async_loop(func):
    """Decorator to ensure async loop exists"""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        if not hasattr(asyncio, 'get_running_loop'):
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        return await func(*args, **kwargs)
    return wrapper

def run_async(coro):
    """Run async function in Streamlit context"""
    try:
        loop = st.session_state.async_loop
        return loop.run_until_complete(coro)
    except Exception as e:
        st.error(f"Async operation failed: {str(e)}")
        raise

================================================================================
# File: .\utils\logging_config.py
================================================================================

import logging
import sys

def setup_logging():
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG)
    
    # Console handler
    ch = logging.StreamHandler(sys.stdout)
    ch.setLevel(logging.DEBUG)
    
    # File handler
    fh = logging.FileHandler('app_debug.log')
    fh.setLevel(logging.DEBUG)

    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    ch.setFormatter(formatter)
    fh.setFormatter(formatter)

    logger.addHandler(ch)
    logger.addHandler(fh)
    return logger

logger = setup_logging()


================================================================================
# File: .\verify.py
================================================================================

import streamlit as st
import langchain
import pinecone
import torch
import transformers
import nltk

print("All critical packages imported successfully!")
